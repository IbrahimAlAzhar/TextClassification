{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"ATTENTION with LSTM + BERT main 16 authors .ipynb","provenance":[{"file_id":"1Sb8VGQ6oOSB4lBkSZG3Ua9Z6kxLKULvb","timestamp":1606748555722}],"collapsed_sections":[],"authorship_tag":"ABX9TyNqMZTKv7sbQQB3shgvDaHD"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J8DHwCFI-KLY","executionInfo":{"status":"ok","timestamp":1606752359456,"user_tz":-360,"elapsed":82770,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"}},"outputId":"40cf982b-48c6-4541-9c1a-90fb15ba27f7"},"source":["!pip install Keras==2.2.4\n","!pip install tensorflow-gpu==1.15\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting Keras==2.2.4\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/10/aa32dad071ce52b5502266b5c659451cfd6ffcbf14e6c8c4f16c0ff5aaab/Keras-2.2.4-py2.py3-none-any.whl (312kB)\n","\r\u001b[K     |█                               | 10kB 20.0MB/s eta 0:00:01\r\u001b[K     |██                              | 20kB 17.1MB/s eta 0:00:01\r\u001b[K     |███▏                            | 30kB 14.5MB/s eta 0:00:01\r\u001b[K     |████▏                           | 40kB 13.1MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 51kB 8.7MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 61kB 9.4MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 71kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 81kB 9.7MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 92kB 9.7MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 102kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 112kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 122kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 133kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 143kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 153kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 163kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 174kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 184kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 194kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 204kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 215kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 225kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 235kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 245kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 256kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 266kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 276kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 286kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 296kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 307kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 317kB 8.4MB/s \n","\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras==2.2.4) (1.1.2)\n","Collecting keras-applications>=1.0.6\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n","\u001b[K     |████████████████████████████████| 51kB 7.4MB/s \n","\u001b[?25hRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras==2.2.4) (1.15.0)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras==2.2.4) (2.10.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras==2.2.4) (3.13)\n","Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from Keras==2.2.4) (1.18.5)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras==2.2.4) (1.4.1)\n","Installing collected packages: keras-applications, Keras\n","  Found existing installation: Keras 2.4.3\n","    Uninstalling Keras-2.4.3:\n","      Successfully uninstalled Keras-2.4.3\n","Successfully installed Keras-2.2.4 keras-applications-1.0.8\n","Collecting tensorflow-gpu==1.15\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/ad/933140e74973fb917a194ab814785e7c23680ca5dee6d663a509fe9579b6/tensorflow_gpu-1.15.0-cp36-cp36m-manylinux2010_x86_64.whl (411.5MB)\n","\u001b[K     |████████████████████████████████| 411.5MB 38kB/s \n","\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (0.2.0)\n","Collecting tensorboard<1.16.0,>=1.15.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n","\u001b[K     |████████████████████████████████| 3.8MB 51.1MB/s \n","\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.1.0)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (0.35.1)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.15.0)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (3.12.4)\n","Collecting gast==0.2.2\n","  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n","Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.0.8)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (3.3.0)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (0.10.0)\n","Collecting tensorflow-estimator==1.15.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n","\u001b[K     |████████████████████████████████| 512kB 47.7MB/s \n","\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.33.2)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.1.2)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.12.1)\n","Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.18.5)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (0.8.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (3.3.3)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (1.0.1)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (50.3.2)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==1.15) (2.10.0)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (2.0.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (3.4.0)\n","Building wheels for collected packages: gast\n","  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7542 sha256=b66ce9a4254695bbe9e2e58beac656ba49705cee4eb52e5e38bbebd3774cc5c8\n","  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n","Successfully built gast\n","\u001b[31mERROR: tensorflow 2.3.0 has requirement gast==0.3.3, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n","\u001b[31mERROR: tensorflow 2.3.0 has requirement tensorboard<3,>=2.3.0, but you'll have tensorboard 1.15.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: tensorflow 2.3.0 has requirement tensorflow-estimator<2.4.0,>=2.3.0, but you'll have tensorflow-estimator 1.15.1 which is incompatible.\u001b[0m\n","\u001b[31mERROR: tensorflow-probability 0.11.0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n","Installing collected packages: tensorboard, gast, tensorflow-estimator, tensorflow-gpu\n","  Found existing installation: tensorboard 2.3.0\n","    Uninstalling tensorboard-2.3.0:\n","      Successfully uninstalled tensorboard-2.3.0\n","  Found existing installation: gast 0.3.3\n","    Uninstalling gast-0.3.3:\n","      Successfully uninstalled gast-0.3.3\n","  Found existing installation: tensorflow-estimator 2.3.0\n","    Uninstalling tensorflow-estimator-2.3.0:\n","      Successfully uninstalled tensorflow-estimator-2.3.0\n","Successfully installed gast-0.2.2 tensorboard-1.15.0 tensorflow-estimator-1.15.1 tensorflow-gpu-1.15.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9FYFELXN-uAN","executionInfo":{"status":"ok","timestamp":1606752379355,"user_tz":-360,"elapsed":3673,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"}},"outputId":"ab1de42d-93c1-407b-ce96-1cf95d8bc98c"},"source":["import os\n","import re\n","import pickle\n","from tqdm import tqdm\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.layers import Dense, Dropout, Embedding, Bidirectional, LSTM\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.callbacks import TensorBoard\n","from sklearn.model_selection import train_test_split\n","# from keras.layers import Embedding\n","from  keras . utils  import  to_categorical\n","from tensorflow.keras.layers import Dense, Input, GlobalMaxPooling1D, Permute\n","from tensorflow.keras.layers import Conv1D,Conv2D, MaxPooling1D, Embedding, Flatten\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.initializers import Constant\n","from tensorflow.keras.layers import Embedding\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from keras.models import Model\n","from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n","from keras.optimizers import RMSprop\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing import sequence\n","from keras.utils import to_categorical\n","from keras.callbacks import EarlyStopping\n","%matplotlib inline\n","\n","\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from keras.models import Model\n","from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n","from keras.optimizers import RMSprop\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing import sequence\n","from keras.utils import to_categorical\n","from keras.callbacks import EarlyStopping\n","%matplotlib inline"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OeCmCz_1_YUA","executionInfo":{"status":"ok","timestamp":1606752434008,"user_tz":-360,"elapsed":51062,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"}},"outputId":"3e82fa88-b9cf-4a9c-fca7-67874e4bf16f"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nhlLBObX_YYs"},"source":["import pandas as pd \n","train_data=pd.read_csv('gdrive/My Drive/Thesis Data/Our dataset/OurDataset_train.csv')\n","stopwords=pd.read_csv('gdrive/My Drive/Colab Notebooks/Stopwords.csv')\n","test_data=pd.read_csv('gdrive/My Drive/Thesis Data/Our dataset/OurDataset_test.csv')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PcqmtsjD_YSJ","executionInfo":{"status":"ok","timestamp":1606752447735,"user_tz":-360,"elapsed":2639,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"}},"outputId":"9e858b9b-fc01-472a-f767-df2d42064b0c"},"source":["!git clone -l -s https://github.com/banglakit/bengali-stemmer.git"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Cloning into 'bengali-stemmer'...\n","warning: --local is ignored\n","remote: Enumerating objects: 25, done.\u001b[K\n","remote: Counting objects: 100% (25/25), done.\u001b[K\n","remote: Compressing objects: 100% (17/17), done.\u001b[K\n","remote: Total 94 (delta 5), reused 16 (delta 4), pack-reused 69\u001b[K\n","Unpacking objects: 100% (94/94), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i-ewyCcu_5zT","executionInfo":{"status":"ok","timestamp":1606752455050,"user_tz":-360,"elapsed":5383,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"}},"outputId":"6b680467-577b-47ce-8ee8-112bb199e028"},"source":["!pip install git+https://github.com/banglakit/bengali-stemmer.git    "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting git+https://github.com/banglakit/bengali-stemmer.git\n","  Cloning https://github.com/banglakit/bengali-stemmer.git to /tmp/pip-req-build-ux5tu9b8\n","  Running command git clone -q https://github.com/banglakit/bengali-stemmer.git /tmp/pip-req-build-ux5tu9b8\n","Building wheels for collected packages: bengali-stemmer\n","  Building wheel for bengali-stemmer (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for bengali-stemmer: filename=bengali_stemmer-0.0.1-py2.py3-none-any.whl size=6393 sha256=f80bf505905bb34b52a39a4faf3f0f292bf5c6a7b22ea217a2c95e8811a7423b\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-j153lfk9/wheels/a1/ad/a1/4ba354059b17c00600a14e13a504e7bdb49f20f2f4e2f3639c\n","Successfully built bengali-stemmer\n","Installing collected packages: bengali-stemmer\n","Successfully installed bengali-stemmer-0.0.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3tTzzCfn_8cC","executionInfo":{"status":"ok","timestamp":1606752460860,"user_tz":-360,"elapsed":4213,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"}},"outputId":"0d6af4f0-7e72-45de-a279-360b68e3b47e"},"source":["!pip install git+https://github.com/banglakit/bengali-stemmer.git    "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting git+https://github.com/banglakit/bengali-stemmer.git\n","  Cloning https://github.com/banglakit/bengali-stemmer.git to /tmp/pip-req-build-vs2b0ai_\n","  Running command git clone -q https://github.com/banglakit/bengali-stemmer.git /tmp/pip-req-build-vs2b0ai_\n","Requirement already satisfied (use --upgrade to upgrade): bengali-stemmer==0.0.1 from git+https://github.com/banglakit/bengali-stemmer.git in /usr/local/lib/python3.6/dist-packages\n","Building wheels for collected packages: bengali-stemmer\n","  Building wheel for bengali-stemmer (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for bengali-stemmer: filename=bengali_stemmer-0.0.1-py2.py3-none-any.whl size=6393 sha256=b83ac63e78ca767ba11e0a0ec2bd29179cd162a4c2b7edd022160a899c21f7bc\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-xipezdo5/wheels/a1/ad/a1/4ba354059b17c00600a14e13a504e7bdb49f20f2f4e2f3639c\n","Successfully built bengali-stemmer\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"vIgSIKgn__pp","executionInfo":{"status":"ok","timestamp":1606752464965,"user_tz":-360,"elapsed":1623,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"}},"outputId":"04757395-5cc3-42f7-ec00-c82b60bb8e73"},"source":["from bengali_stemmer.rafikamal2014 import RafiStemmer\n","stemmer = RafiStemmer()\n","stemmer.stem_word('বাংলায়')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'বাংলা'"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"gtztOzCFADCB"},"source":["total_data = train_data\n","total_data=total_data.append(test_data, ignore_index = True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tbY3EyqpAJF5"},"source":["macronum=sorted(set(total_data['label']))\n","macro_to_id = dict((note, number) for number, note in enumerate(macronum))\n","\n","def fun(i):\n","    return macro_to_id[i]\n","\n","total_data['label']=total_data['label'].apply(fun)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nQcuEgS8ANky"},"source":["texts = list(total_data['text'])\n","labels = list(total_data['label'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B6ySqe-9AQra"},"source":["def clean_punct(sentence):\n","    cleaned = re.sub(r'[?|!|\\'|\"|#|।|’|‘]', r'', sentence)\n","    cleaned1 = re.sub(r'[.|,|(|)|\\|/]', r'', cleaned)\n","    cleaned = re.sub(r'[০|১|২|৩|৪|৫|৬|৭|৮|৯]', r'', cleaned1)\n","    cleaned1 = re.sub(r'[-|=]', r' ', cleaned)\n","    return cleaned1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A5j-I80QATn6"},"source":["set_stop = set(stopwords['words'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XNvbzRt4AV_6"},"source":["\n","def pre_process(data):\n","    i=0\n","    str1=' '\n","    final_string = []\n","    final_words = []\n","    all_negative_words = []\n","    s=''\n","\n","    for sentence in data:\n","        filtered_sentence = []\n","\n","        for w in sentence.split():\n","            for cleaned_word in clean_punct(w).split():\n","                if len(cleaned_word)>2:\n","                    if((cleaned_word) not in set_stop):\n","                        s = stemmer.stem_word(cleaned_word)\n","                        if len(s)>2:\n","                            final_words.append(s)\n","                            filtered_sentence.append(s)\n","                    else:\n","                        continue\n","                else:\n","                    continue\n","\n","        str1 = \" \".join(filtered_sentence)\n","        final_string.append(str1)\n","    return final_string"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JHvdUxDfAYzC"},"source":["texts = pre_process(texts)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":120},"id":"BItsM47cDwYi","executionInfo":{"status":"ok","timestamp":1606752691439,"user_tz":-360,"elapsed":1364,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"}},"outputId":"0209f4f9-5e08-48eb-e118-ee9c4aa0aa6a"},"source":["texts[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'দূর ভরত হেস উঠল আপন বালিকা বধূ স্কুল পাঠানো সরলা বিবির টানাটানি সরলা বেথুন স্কুল লোরে হাউজ দুজন ধারণা স্কুল ভালো বেথুন স্কুল বাঙা পাড়া বাংলা মাধ্যম লোরেটো হাউজ সাহেব পাড় ইংরেজি স্কুল বাঙা ছাত্রীর তুলনা ফিরিঙ্গি ছাত্রী বেথুন ছাত্রী সমাজ নাম মহিলা বছর বেথুন ছাত্রী হইচ প্রবেশিকা পরীক্ষা ভালোভাব উত্তীর্ণ অবলা দাস নাম মেয়ে ডাক্তারি পড়ব কলকাতা মেডিক কলেজ ছাত্রী নেয়া অবল কেনো ডাক্তারি পারব শেষ পাঠি মাদ্রাজ মেডিক কলেজ অবল জেদ বাঙলা সরক কুড়ি টাক মাসিক বৃত্তির অনুমোদন দেশ কোথা আন্দোলন বেথুন কলেজ প্রভাব ইলবার্ট বিল সাহেব দেশী মানুষ বিদ্য বুদ্ধি প্রচ লাগল কামিনী নাম তেজস্বিনী ছাত্রীর নেতৃত্ব বেথুন বিক্ষোভ জানি সুর বাড়ুজ্য যেদিন সেদিন বেথুন ছাত্রী কালো বেধ স্কুল লোরেটো হাউজ এসব স্বদেশিয়ানা নিষিদ্ধ প্রভু যিশুর জয়গান নিয়মিত প্রার্থনা ছাত্রী ভালো ইংরেজি শেখ বিলিতি আদব কায়দা রপ্ত পাস ব্যারিস্ পত্নী হিসেব মানি বিবির বয়েস বছর সরল এগারো মামা পিসতু বোন ভাব মাঝ মাঝ তর্ক বয়েস সরল ইংরেজ শাসন সম্পর্ রাগ রাগ ভাব প্রা আবৃত্তি স্বাধীনতা হীনতা বাঁচি বাঁচি বয়েসী পরিবার নিয়ম কাকিমা মামী সরলা ছুট দুজন হাত ইকুল ভর্তি বলো আমার ভালো বউয় আড়ষ্টতা কাটেনি এমনকি বিবাহ নাম মৃণালিনী যশোর গ্রাম ভবতারিণী প্রাসাদ মানুষজন দাস দাসী দিশেহা অবস্থা রূপকথ মতন কুঁড়েঘর রাজবাড়ির বধূ রাজপুত্র মতন রূপবান স্বামী ভালো ভাব হেমেন্দ্রনাথ স্ত্রী নীপময়ীর থাক ব্যবস্থা নীপময়ী বাড়ির রীতিনীতি শেখাচ্ছ বিয় দিন কয়েকদিন উৎসব রেশ থাক কথা বাড়ি শোক ছায়া রবির বিবাহ রাত্র শিলাইদহ সারদাপ্রসাদ গাঙ্গুলির হঠাৎ হৃদরোগ মৃত্যু দিদি সৌদামিনী রবির মায় মতন তিনি সংসার কর্ত্রী সময় সত্যেন্দ্রনাথ খবর তিন পৌঁছ মূৰ্ছা যাচ্ছ বাড়ি সবা ফিসফ কথা একমাত্র বাচ্চা নিয়ম মান রবির স্ত্রী কোথা শিক্ষা গ্ৰহণ সরল তর্ক মূল্য আসল সিদ্ধান্ত নেব জ্ঞানদানন্দিনী ফেল এবার সার্কুল রোড বাড়ি ভাড়া মৃণালিনী সেখান বাড়ি লোরেটো হাউজ দূর বিবির সঙ্গ পারব গাড়ি শাড়ি টাড়ি চলব উত্তম বিলিতি কাপড় কিন স্কার্ট বানানো লাগল ব্যবস্থা অনেক মনঃপূত কাদম্বরীর সাধ স্ত্রী সঙ্গিনী মতন মেয়ে তুলব রবির আলাদা মহল নির্দিষ্ট সাজানো গোছানো সুন্দরভাব বাড়ি বেথুন স্কুল বাড়ির তিন স্কুল পাঠানোর অসুবিধ জ্ঞানদানন্দিনীর ছেল কাদম্বরীর রবির কেড় নেব জ্ঞানদানন্দিনী বাড়ি ভর্তি মানুষ গমগম রবির নিভৃ কথা সুযোগ অন্য সামন কাদম্বরী সহজভাব জিজ্ঞেস ছোট বেথুন পড়াল ভালো বাংল ইস্কুল বিব্রতভাব বলল মেজ বউঠান করলেন… নীপময়ী উপস্থিত তোর অক্ষর ইংরেজি জান কথা দেখ প্রাইমারী ইকুল বাংলা একটু আধটু শিখ ইংরেজির অক্ষরজ্ঞান লোরেটোর ফিরিঙ্গি মেয়ে পড়াশুনো পারব এক্ষুনি ইস্কুল পাঠাব দরক থাক প্রথম শিখি পড়ি দেব তোম একটু মেজ বউঠান বুঝি বলো কথা নীপাময়ী ঝংক তোর বউয় যাব পার नां রবির মুখ দেখ বোঝা জ্ঞানদানন্দিনীর এরকম প্রস্তাব তোল সাহস সঞ্চ পারব কাদম্বরী নিঃশব্দ ছায় মতন বেথুন ইস্কুল বাংল গর্ব বিরোধিতা অগ্রাহ্য এদেশ মেয়ে শিক্ষ প্রতিষ্ঠা বিদ্যাসাগর'"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"f_R0i3i1DwoM"},"source":["def load_data(num_words, sequence_length, test_size=0.25, oov_token=None):\n","    tokenizer = Tokenizer(num_words=num_words, oov_token=oov_token)\n","    num_classes=16\n","    y = to_categorical(np.asarray(labels), num_classes)\n","    tokenizer.fit_on_texts(texts)\n","    X = tokenizer.texts_to_sequences(texts)\n","    X = np.array(X)\n","    # pad sequences with 0's\n","    X = pad_sequences(X, maxlen=sequence_length)\n","    # split data to training and testing sets\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=1)\n","    data = {}\n","    data[\"X_train\"] = X_train\n","    data[\"X_test\"]= X_test\n","    data[\"y_train\"] = y_train\n","    data[\"y_test\"] = y_test\n","    data[\"tokenizer\"] = tokenizer\n","    return data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bUbDt56IsV6Q","executionInfo":{"status":"ok","timestamp":1606752706961,"user_tz":-360,"elapsed":6757,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"}},"outputId":"44da95bf-3ad0-4f29-e851-94f85a97588a"},"source":["!pip install transformers"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/83/e74092e7f24a08d751aa59b37a9fc572b2e4af3918cb66f7766c3affb1b4/transformers-3.5.1-py3-none-any.whl (1.3MB)\n","\u001b[K     |████████████████████████████████| 1.3MB 8.2MB/s \n","\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 28.9MB/s \n","\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n","Collecting sentencepiece==0.1.91\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 40.9MB/s \n","\u001b[?25hCollecting tokenizers==0.9.3\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/34/b39eb9994bc3c999270b69c9eea40ecc6f0e97991dba28282b9fd32d44ee/tokenizers-0.9.3-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n","\u001b[K     |████████████████████████████████| 2.9MB 54.9MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.11.8)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=fd9eaa094ba0aafaa673dcf59b5cb18f80266ff4e3844f55af1acd01a214fedf\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: sacremoses, sentencepiece, tokenizers, transformers\n","Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.9.3 transformers-3.5.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yv5Pgf0Er7V7","executionInfo":{"status":"ok","timestamp":1606752985954,"user_tz":-360,"elapsed":10746,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"}},"outputId":"101cda40-7a0b-4763-e75a-a4cd34bb75a9"},"source":["from transformers import BertForMaskedLM, BertTokenizer, pipeline\n","\n","model = BertForMaskedLM.from_pretrained(\"sagorsarker/bangla-bert-base\")\n","tokenizer = BertTokenizer.from_pretrained(\"sagorsarker/bangla-bert-base\")\n","nlp = pipeline('fill-mask', model=model, tokenizer=tokenizer)\n","for pred in nlp(f\"আমি বাংলায় {nlp.tokenizer.mask_token} গাই।\"):\n","  print(pred)\n","\n","# {'sequence': '[CLS] আমি বাংলায গান গাই । [SEP]', 'score': 0.13404667377471924, 'token': 2552, 'token_str': 'গান'}"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Some weights of the model checkpoint at sagorsarker/bangla-bert-base were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n","- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"KbApwCBzDwVO"},"source":["'''\n","# train_data=pd.read_csv('gdrive/My Drive/Colab Notebooks/ulm_train.csv')\n","import numpy as np\n","\n","def get_embedding_vectors(word_index, embedding_size=100):\n","    \n","    embedding_matrix = np.zeros((len(word_index) + 1, embedding_size))\n","    with open(f\"gdrive/My Drive/Colab Notebooks/bn_glove.{embedding_size}d.txt\", encoding=\"utf8\") as f:\n","        for line in tqdm(f, \"Reading GloVe\"):\n","            values = line.split()\n","            # get the word as the first word in the line\n","            word = values[0]\n","            if word in word_index:\n","                idx = word_index[word]\n","                # get the vectors as the remaining values in the line\n","                embedding_matrix[idx] = np.array(values[1:], dtype=\"float32\")\n","    return embedding_matrix\n","'''"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2a0vCRMxEhfZ"},"source":["d = set()\n","for s in texts:\n","    for ss in list(s.split()):\n","        d.add(ss)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r_XIlZWTFWHB","executionInfo":{"status":"ok","timestamp":1606626652311,"user_tz":-360,"elapsed":2037,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"}},"outputId":"477ab737-d8c3-4b8f-a9b3-2e7ec2073991"},"source":["len(d)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["284632"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bh6RPvx6FY4g","executionInfo":{"status":"ok","timestamp":1606626654680,"user_tz":-360,"elapsed":1529,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"}},"outputId":"c2c7591b-3297-44e2-f299-b86b27f8de48"},"source":["max([len(s.split()) for s in texts]) # find the max length"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["606"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":163},"id":"VXppkk2SHxgg","executionInfo":{"status":"error","timestamp":1605981728473,"user_tz":-360,"elapsed":894,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"}},"outputId":"508ead40-d9b6-4fdb-ccd3-c67d1ace8b81"},"source":["len(data['tokenizer'].word_index)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-24-76422d5f051f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokenizer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"]}]},{"cell_type":"code","metadata":{"id":"YTzBc1JPFx6o"},"source":["MAX_SEQUENCE_LENGTH = 606    #max([len(s.split()) for s in texts]) \n","MAX_NUM_WORDS = 280432 + 1  # 'MAX_NUM_WORDS' is length of tokenizer + 1\n","vocab_size = MAX_NUM_WORDS\n","EMBEDDING_DIM = 300\n","VALIDATION_SPLIT = 0.2\n","\n","data = load_data(MAX_NUM_WORDS , MAX_SEQUENCE_LENGTH,VALIDATION_SPLIT)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kBVNWuUoKFfL","executionInfo":{"status":"ok","timestamp":1606627077224,"user_tz":-360,"elapsed":9778,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"}},"outputId":"d049f5e2-2aec-4e8f-8445-e6a6b89248ec"},"source":["embedding_matrix = get_embedding_vectors( data['tokenizer'].word_index ,EMBEDDING_DIM )"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Reading GloVe: 134256it [00:08, 16217.19it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"lcIO0YiVKFxN"},"source":["max_words = 280432 + 1\n","# max_len = 11276\n","# tok = Tokenizer(num_words=max_words)\n","# tok.fit_on_texts(X_train)\n","# sequences = tok.texts_to_sequences(X_train)\n","# sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QSgFWmdRKFkD"},"source":["from keras import backend as K\n","from keras.engine.topology import Layer\n","from keras import initializers, regularizers, constraints\n","\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU \n","from keras.layers import GRU, LSTM, BatchNormalization\n","from keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten\n","from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\n","from keras.models import Model, load_model\n","from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n","from keras import backend as K\n","from keras.engine import InputSpec, Layer\n","from keras.optimizers import Adam\n","\n","from keras.callbacks import ModelCheckpoint, TensorBoard, Callback, EarlyStopping"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qsWvHgw1KFbG"},"source":["class Attention(Layer):\n","    \"\"\"\n","    Keras Layer that implements an Attention mechanism for temporal data.\n","    Supports Masking.\n","    Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n","    # Input shape\n","        3D tensor with shape: `(samples, steps, features)`.\n","    # Output shape\n","        2D tensor with shape: `(samples, features)`.\n","    :param kwargs:\n","    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n","    The dimensions are inferred based on the output shape of the RNN.\n","    Example:\n","        model.add(LSTM(64, return_sequences=True))\n","        model.add(Attention())\n","    \"\"\"   \n","   \n","   \n","    def __init__(self, step_dim,\n","                 W_regularizer=None, b_regularizer=None,\n","                 W_constraint=None, b_constraint=None,\n","                 bias=True, **kwargs):\n","        self.supports_masking = True\n","        self.init = initializers.get('glorot_uniform')\n","\n","        self.W_regularizer = regularizers.get(W_regularizer)\n","        self.b_regularizer = regularizers.get(b_regularizer)\n","\n","        self.W_constraint = constraints.get(W_constraint)\n","        self.b_constraint = constraints.get(b_constraint)\n","\n","        self.bias = bias\n","        self.step_dim = step_dim\n","        self.features_dim = 0\n","        super(Attention, self).__init__(**kwargs)          \n","            \n","            \n","        self.features_dim = 0\n","        super(Attention, self).__init__(**kwargs)\n","\n","    def build(self, input_shape):\n","        assert len(input_shape) == 3\n","\n","        self.W = self.add_weight((input_shape[-1],),\n","                                 initializer=self.init,\n","                                 name='{}_W'.format(self.name),\n","                                 regularizer=self.W_regularizer,\n","                                 constraint=self.W_constraint)\n","        self.features_dim = input_shape[-1]\n","\n","        if self.bias:\n","            self.b = self.add_weight((input_shape[1],),\n","                                     initializer='zero',\n","                                     name='{}_b'.format(self.name),\n","                                     regularizer=self.b_regularizer,\n","                                     constraint=self.b_constraint)\n","        else:\n","            self.b = None\n","        self.built = True\n","    \n","    def compute_mask(self, input, input_mask=None):\n","        return None\n","\n","    def call(self, x, mask=None):\n","        features_dim = self.features_dim\n","        step_dim = self.step_dim\n","        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n","                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n","        if self.bias:\n","            eij += self.b\n","        eij = K.tanh(eij)\n","        a = K.exp(eij)\n","        if mask is not None:\n","            a *= K.cast(mask, K.floatx())\n","        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n","        a = K.expand_dims(a)\n","        weighted_input = x * a\n","        return K.sum(weighted_input, axis=1)\n","\n","    def compute_output_shape(self, input_shape):\n","        return input_shape[0],  self.features_dim"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kFnVIiJqKfa1"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"g3npxFC_KlWH"},"source":["max_len = 606 (as usual), bs=64"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"PKpn49hPKFZj","executionInfo":{"status":"ok","timestamp":1606627099134,"user_tz":-360,"elapsed":4642,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"}},"outputId":"b081e248-b6fa-464f-d7db-7ce0395e0845"},"source":["#max_features = 30000\n","max_features = 280432 + 1\n","max_len = 606\n","def build_model(max_len, max_features, EMBEDDING_DIM, embedding_matrix):\n","    input_words = Input((max_len, ))\n","    x_words = Embedding(max_features,\n","                        EMBEDDING_DIM,\n","                        weights=[embedding_matrix],\n","                        mask_zero=True,\n","                        trainable=False)(input_words)\n","    x_words = SpatialDropout1D(0.2)(x_words)\n","    x_words = Bidirectional(LSTM(128, return_sequences=True))(x_words)\n","    x_words = Bidirectional(LSTM(128, return_sequences=True))(x_words)\n","    \n","    x = Attention(max_len)(x_words)\n","    #x = GlobalMaxPooling1D()(x)\n","    #x = GlobalAveragePooling1D()(x)\n","    x = Dropout(0.2)(x)\n","    x = Dense(64, activation='relu')(x)\n","    x = Dropout(0.2)(x)\n","    pred = Dense(16, activation='softmax')(x)\n","\n","    model = Model(inputs=input_words, outputs=pred)\n","    return model\n","\n","model = build_model(max_len, max_features, EMBEDDING_DIM, embedding_matrix)\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","model.summary()\n","'''\n","MAX_SEQUENCE_LENGTH = 11276    #max([len(s.split()) for s in texts]) \n","MAX_NUM_WORDS = 109803 + 1\n","vocab_size = MAX_NUM_WORDS\n","EMBEDDING_DIM = 300\n","VALIDATION_SPLIT = 0.2\n","\n","data = load_data(MAX_NUM_WORDS , MAX_SEQUENCE_LENGTH,VALIDATION_SPLIT)\n","'''"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2974: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n","\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_1 (InputLayer)         (None, 606)               0         \n","_________________________________________________________________\n","embedding_1 (Embedding)      (None, 606, 300)          84129900  \n","_________________________________________________________________\n","spatial_dropout1d_1 (Spatial (None, 606, 300)          0         \n","_________________________________________________________________\n","bidirectional_1 (Bidirection (None, 606, 256)          439296    \n","_________________________________________________________________\n","bidirectional_2 (Bidirection (None, 606, 256)          394240    \n","_________________________________________________________________\n","attention_2 (Attention)      (None, 256)               862       \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 256)               0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 64)                16448     \n","_________________________________________________________________\n","dropout_2 (Dropout)          (None, 64)                0         \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 16)                1040      \n","=================================================================\n","Total params: 84,981,786\n","Trainable params: 851,886\n","Non-trainable params: 84,129,900\n","_________________________________________________________________\n"],"name":"stdout"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\nMAX_SEQUENCE_LENGTH = 11276    #max([len(s.split()) for s in texts]) \\nMAX_NUM_WORDS = 109803 + 1\\nvocab_size = MAX_NUM_WORDS\\nEMBEDDING_DIM = 300\\nVALIDATION_SPLIT = 0.2\\n\\ndata = load_data(MAX_NUM_WORDS , MAX_SEQUENCE_LENGTH,VALIDATION_SPLIT)\\n'"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"code","metadata":{"id":"hd7rdT4nKtwI"},"source":["import tensorflow as tf\n","callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yz7wLKRbKt-O","outputId":"ac4db016-27ee-4d5e-aa90-81142c38bcc1"},"source":["# use only 10 epochs\n","history = model.fit(data['X_train'], data['y_train'], validation_data=(data['X_test'], data['y_test']), epochs=30, verbose=2, batch_size=64)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n","\n","Train on 14372 samples, validate on 3594 samples\n","Epoch 1/30\n"," - 1150s - loss: 1.5275 - acc: 0.5098 - val_loss: 0.8419 - val_acc: 0.7309\n","Epoch 2/30\n"," - 1147s - loss: 0.7804 - acc: 0.7410 - val_loss: 0.4950 - val_acc: 0.8306\n","Epoch 3/30\n"," - 1148s - loss: 0.5086 - acc: 0.8335 - val_loss: 0.3878 - val_acc: 0.8854\n","Epoch 4/30\n"," - 1150s - loss: 0.3715 - acc: 0.8840 - val_loss: 0.2789 - val_acc: 0.9129\n","Epoch 5/30\n"," - 1175s - loss: 0.2745 - acc: 0.9137 - val_loss: 0.2433 - val_acc: 0.9277\n","Epoch 6/30\n"," - 1182s - loss: 0.2228 - acc: 0.9310 - val_loss: 0.2067 - val_acc: 0.9382\n","Epoch 7/30\n"," - 1162s - loss: 0.1839 - acc: 0.9416 - val_loss: 0.1874 - val_acc: 0.9416\n","Epoch 8/30\n"," - 1175s - loss: 0.1486 - acc: 0.9540 - val_loss: 0.1835 - val_acc: 0.9477\n","Epoch 9/30\n"," - 1156s - loss: 0.1265 - acc: 0.9604 - val_loss: 0.1606 - val_acc: 0.9519\n","Epoch 10/30\n"," - 1204s - loss: 0.1066 - acc: 0.9675 - val_loss: 0.1321 - val_acc: 0.9599\n","Epoch 11/30\n"," - 1199s - loss: 0.0863 - acc: 0.9733 - val_loss: 0.1382 - val_acc: 0.9608\n","Epoch 12/30\n"," - 1166s - loss: 0.0897 - acc: 0.9711 - val_loss: 0.1205 - val_acc: 0.9641\n","Epoch 13/30\n"," - 1149s - loss: 0.0618 - acc: 0.9818 - val_loss: 0.1191 - val_acc: 0.9652\n","Epoch 14/30\n"," - 1142s - loss: 0.0569 - acc: 0.9832 - val_loss: 0.1591 - val_acc: 0.9519\n","Epoch 15/30\n"," - 1141s - loss: 0.0518 - acc: 0.9840 - val_loss: 0.1011 - val_acc: 0.9705\n","Epoch 16/30\n"," - 1197s - loss: 0.0462 - acc: 0.9856 - val_loss: 0.1146 - val_acc: 0.9674\n","Epoch 17/30\n"," - 1225s - loss: 0.0478 - acc: 0.9855 - val_loss: 0.1024 - val_acc: 0.9741\n","Epoch 18/30\n"," - 1164s - loss: 0.0401 - acc: 0.9872 - val_loss: 0.1221 - val_acc: 0.9658\n","Epoch 19/30\n"," - 1150s - loss: 0.0460 - acc: 0.9862 - val_loss: 0.1435 - val_acc: 0.9616\n","Epoch 20/30\n"," - 1141s - loss: 0.0342 - acc: 0.9900 - val_loss: 0.0941 - val_acc: 0.9777\n","Epoch 21/30\n"," - 1136s - loss: 0.0271 - acc: 0.9912 - val_loss: 0.0953 - val_acc: 0.9747\n","Epoch 22/30\n"," - 1146s - loss: 0.0391 - acc: 0.9872 - val_loss: 0.1254 - val_acc: 0.9599\n","Epoch 23/30\n"," - 1138s - loss: 0.0307 - acc: 0.9905 - val_loss: 0.0964 - val_acc: 0.9725\n","Epoch 24/30\n"," - 1141s - loss: 0.0231 - acc: 0.9937 - val_loss: 0.1026 - val_acc: 0.9719\n","Epoch 25/30\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Fd-ev45UKtrn"},"source":["import seaborn as sns\n","from sklearn.metrics import confusion_matrix\n","import matplotlib.pyplot as plt\n","def confussion_mat(model):\n","    # Predict the values from the validation dataset\n","    Y_pred = model.predict(data['X_test'])\n","    # Convert predictions classes to one hot vectors \n","    Y_pred_classes = np.argmax(Y_pred,axis = 1) \n","    # Convert validation observations to one hot vectors\n","    Y_true = np.argmax(data['y_test'],axis = 1) \n","    # compute the confusion matrix\n","    confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n","    # plot the confusion matrix\n","    f,ax = plt.subplots(figsize=(15, 15))\n","    sns.heatmap(confusion_mtx, annot=True, linewidths=0.01,cmap=\"Greens\",linecolor=\"gray\", fmt= '.1f',ax=ax)\n","    plt.xlabel(\"Predicted Label\")\n","    plt.ylabel(\"True Label\")\n","    plt.title(\"Confusion Matrix\")\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RE5aEW-dKtqK"},"source":["def history2(history):\n","    # list all data in history\n","    print(history.history.keys())\n","    # summarize history for accuracy\n","    plt.plot(history.history['acc'])\n","    plt.plot(history.history['val_acc'])\n","    plt.title('model accuracy')\n","    plt.ylabel('accuracy')\n","    plt.xlabel('epoch')\n","    plt.legend(['train', 'test'], loc='upper left')\n","    plt.show()\n","    # summarize history for loss\n","    plt.plot(history.history['loss'])\n","    plt.plot(history.history['val_loss'])\n","    plt.title('model loss')\n","    plt.ylabel('loss')\n","    plt.xlabel('epoch')\n","    plt.legend(['train', 'test'], loc='upper left')\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":163},"id":"F4CNvAmAcUuA","executionInfo":{"status":"error","timestamp":1606659445971,"user_tz":-360,"elapsed":1269,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"}},"outputId":"21d46920-945f-482b-9a3b-845234722caf"},"source":["history2(history)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-d0bdc55723b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"]}]},{"cell_type":"code","metadata":{"id":"jLyzsNsCcUmH"},"source":["confussion_mat(model)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TcJaElCjKgvq"},"source":[""],"execution_count":null,"outputs":[]}]}