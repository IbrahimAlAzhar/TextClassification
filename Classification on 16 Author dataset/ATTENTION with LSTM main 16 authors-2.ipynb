{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"ATTENTION with LSTM main 16 authors-2.ipynb","provenance":[{"file_id":"1Sb8VGQ6oOSB4lBkSZG3Ua9Z6kxLKULvb","timestamp":1606926039592}],"collapsed_sections":[],"authorship_tag":"ABX9TyMK1Blfixii740J/RKD2rZw"},"kernelspec":{"display_name":"Python 3","name":"python3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J8DHwCFI-KLY","executionInfo":{"status":"ok","timestamp":1606910531287,"user_tz":-360,"elapsed":65950,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"}},"outputId":"c8b89261-c907-4878-b5fc-12ef78d7d256"},"source":["!pip install Keras==2.2.4\n","!pip install tensorflow-gpu==1.15\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting Keras==2.2.4\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/10/aa32dad071ce52b5502266b5c659451cfd6ffcbf14e6c8c4f16c0ff5aaab/Keras-2.2.4-py2.py3-none-any.whl (312kB)\n","\u001b[K     |████████████████████████████████| 317kB 4.9MB/s \n","\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras==2.2.4) (3.13)\n","Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from Keras==2.2.4) (1.18.5)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras==2.2.4) (1.15.0)\n","Collecting keras-applications>=1.0.6\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n","\u001b[K     |████████████████████████████████| 51kB 5.2MB/s \n","\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras==2.2.4) (1.1.2)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras==2.2.4) (2.10.0)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras==2.2.4) (1.4.1)\n","Installing collected packages: keras-applications, Keras\n","  Found existing installation: Keras 2.4.3\n","    Uninstalling Keras-2.4.3:\n","      Successfully uninstalled Keras-2.4.3\n","Successfully installed Keras-2.2.4 keras-applications-1.0.8\n","Collecting tensorflow-gpu==1.15\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/ad/933140e74973fb917a194ab814785e7c23680ca5dee6d663a509fe9579b6/tensorflow_gpu-1.15.0-cp36-cp36m-manylinux2010_x86_64.whl (411.5MB)\n","\u001b[K     |████████████████████████████████| 411.5MB 31kB/s \n","\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (0.2.0)\n","Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.0.8)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.15.0)\n","Collecting tensorboard<1.16.0,>=1.15.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n","\u001b[K     |████████████████████████████████| 3.8MB 44.4MB/s \n","\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (3.3.0)\n","Collecting tensorflow-estimator==1.15.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n","\u001b[K     |████████████████████████████████| 512kB 45.2MB/s \n","\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.33.2)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (0.35.1)\n","Collecting gast==0.2.2\n","  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.12.1)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (3.12.4)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.1.2)\n","Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.18.5)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (1.1.0)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (0.10.0)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.15) (0.8.1)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==1.15) (2.10.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (1.0.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (3.3.3)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (50.3.2)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (2.0.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (3.4.0)\n","Building wheels for collected packages: gast\n","  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7542 sha256=f4b3efd55290c8bcfd9231a7becca11f37c242b5831b5c0b46993bdaac0683b0\n","  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n","Successfully built gast\n","\u001b[31mERROR: tensorflow 2.3.0 has requirement gast==0.3.3, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n","\u001b[31mERROR: tensorflow 2.3.0 has requirement tensorboard<3,>=2.3.0, but you'll have tensorboard 1.15.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: tensorflow 2.3.0 has requirement tensorflow-estimator<2.4.0,>=2.3.0, but you'll have tensorflow-estimator 1.15.1 which is incompatible.\u001b[0m\n","\u001b[31mERROR: tensorflow-probability 0.11.0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n","Installing collected packages: tensorboard, tensorflow-estimator, gast, tensorflow-gpu\n","  Found existing installation: tensorboard 2.3.0\n","    Uninstalling tensorboard-2.3.0:\n","      Successfully uninstalled tensorboard-2.3.0\n","  Found existing installation: tensorflow-estimator 2.3.0\n","    Uninstalling tensorflow-estimator-2.3.0:\n","      Successfully uninstalled tensorflow-estimator-2.3.0\n","  Found existing installation: gast 0.3.3\n","    Uninstalling gast-0.3.3:\n","      Successfully uninstalled gast-0.3.3\n","Successfully installed gast-0.2.2 tensorboard-1.15.0 tensorflow-estimator-1.15.1 tensorflow-gpu-1.15.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9FYFELXN-uAN"},"source":["import os\n","import re\n","import pickle\n","from tqdm import tqdm\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.layers import Dense, Dropout, Embedding, Bidirectional, LSTM\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.callbacks import TensorBoard\n","from sklearn.model_selection import train_test_split\n","# from keras.layers import Embedding\n","from  keras . utils  import  to_categorical\n","from tensorflow.keras.layers import Dense, Input, GlobalMaxPooling1D, Permute\n","from tensorflow.keras.layers import Conv1D,Conv2D, MaxPooling1D, Embedding, Flatten\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.initializers import Constant\n","from tensorflow.keras.layers import Embedding\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from keras.models import Model\n","from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n","from keras.optimizers import RMSprop\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing import sequence\n","from keras.utils import to_categorical\n","from keras.callbacks import EarlyStopping\n","%matplotlib inline\n","\n","\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from keras.models import Model\n","from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n","from keras.optimizers import RMSprop\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing import sequence\n","from keras.utils import to_categorical\n","from keras.callbacks import EarlyStopping\n","%matplotlib inline"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OeCmCz_1_YUA","executionInfo":{"status":"ok","timestamp":1606910664981,"user_tz":-360,"elapsed":84042,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"}},"outputId":"017d75f4-3cef-4ad4-ae9f-caec589eda88"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nhlLBObX_YYs"},"source":["import pandas as pd \n","train_data=pd.read_csv('gdrive/My Drive/Thesis Data/Our dataset/OurDataset_train.csv')\n","stopwords=pd.read_csv('gdrive/My Drive/Colab Notebooks/Stopwords.csv')\n","test_data=pd.read_csv('gdrive/My Drive/Thesis Data/Our dataset/OurDataset_test.csv')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PcqmtsjD_YSJ","executionInfo":{"status":"ok","timestamp":1606910684982,"user_tz":-360,"elapsed":6049,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"}},"outputId":"f8758097-f034-4caf-fd58-f54a7ea3b0ec"},"source":["!git clone -l -s https://github.com/banglakit/bengali-stemmer.git"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Cloning into 'bengali-stemmer'...\n","warning: --local is ignored\n","remote: Enumerating objects: 25, done.\u001b[K\n","remote: Counting objects: 100% (25/25), done.\u001b[K\n","remote: Compressing objects: 100% (17/17), done.\u001b[K\n","remote: Total 94 (delta 5), reused 16 (delta 4), pack-reused 69\u001b[K\n","Unpacking objects: 100% (94/94), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i-ewyCcu_5zT","executionInfo":{"status":"ok","timestamp":1606910709198,"user_tz":-360,"elapsed":23363,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"}},"outputId":"c67b6101-4cad-471a-a296-be4f46f096c1"},"source":["!pip install git+https://github.com/banglakit/bengali-stemmer.git    "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting git+https://github.com/banglakit/bengali-stemmer.git\n","  Cloning https://github.com/banglakit/bengali-stemmer.git to /tmp/pip-req-build-l_ayx68i\n","  Running command git clone -q https://github.com/banglakit/bengali-stemmer.git /tmp/pip-req-build-l_ayx68i\n","Building wheels for collected packages: bengali-stemmer\n","  Building wheel for bengali-stemmer (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for bengali-stemmer: filename=bengali_stemmer-0.0.1-py2.py3-none-any.whl size=6393 sha256=64df2068e4317588c12259c2f9f58027cd85b490940005effcb7269566e71ab7\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-9x4j_qa_/wheels/a1/ad/a1/4ba354059b17c00600a14e13a504e7bdb49f20f2f4e2f3639c\n","Successfully built bengali-stemmer\n","Installing collected packages: bengali-stemmer\n","Successfully installed bengali-stemmer-0.0.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3tTzzCfn_8cC","executionInfo":{"status":"ok","timestamp":1606910717816,"user_tz":-360,"elapsed":7638,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"}},"outputId":"6ac47f35-177b-4f1b-ce56-4571b5755663"},"source":["!pip install git+https://github.com/banglakit/bengali-stemmer.git    "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting git+https://github.com/banglakit/bengali-stemmer.git\n","  Cloning https://github.com/banglakit/bengali-stemmer.git to /tmp/pip-req-build-ws2d4qx4\n","  Running command git clone -q https://github.com/banglakit/bengali-stemmer.git /tmp/pip-req-build-ws2d4qx4\n","Requirement already satisfied (use --upgrade to upgrade): bengali-stemmer==0.0.1 from git+https://github.com/banglakit/bengali-stemmer.git in /usr/local/lib/python3.6/dist-packages\n","Building wheels for collected packages: bengali-stemmer\n","  Building wheel for bengali-stemmer (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for bengali-stemmer: filename=bengali_stemmer-0.0.1-py2.py3-none-any.whl size=6393 sha256=a9586b46c374f9f1eca763479c14356c63b92eacc0c4ea272ee5e6b38e5718fc\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-1r4na69k/wheels/a1/ad/a1/4ba354059b17c00600a14e13a504e7bdb49f20f2f4e2f3639c\n","Successfully built bengali-stemmer\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"vIgSIKgn__pp","executionInfo":{"status":"ok","timestamp":1606910720508,"user_tz":-360,"elapsed":1806,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"}},"outputId":"6b33ac43-b05e-4b3a-9fbc-76c424843efb"},"source":["from bengali_stemmer.rafikamal2014 import RafiStemmer\n","stemmer = RafiStemmer()\n","stemmer.stem_word('বাংলায়')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'বাংলা'"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"gtztOzCFADCB"},"source":["total_data = train_data\n","total_data=total_data.append(test_data, ignore_index = True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tbY3EyqpAJF5"},"source":["macronum=sorted(set(total_data['label']))\n","macro_to_id = dict((note, number) for number, note in enumerate(macronum))\n","\n","def fun(i):\n","    return macro_to_id[i]\n","\n","total_data['label']=total_data['label'].apply(fun)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nQcuEgS8ANky"},"source":["texts = list(total_data['text'])\n","labels = list(total_data['label'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B6ySqe-9AQra"},"source":["def clean_punct(sentence):\n","    cleaned = re.sub(r'[?|!|\\'|\"|#|।|’|‘]', r'', sentence)\n","    cleaned1 = re.sub(r'[.|,|(|)|\\|/]', r'', cleaned)\n","    cleaned = re.sub(r'[০|১|২|৩|৪|৫|৬|৭|৮|৯]', r'', cleaned1)\n","    cleaned1 = re.sub(r'[-|=]', r' ', cleaned)\n","    return cleaned1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A5j-I80QATn6"},"source":["set_stop = set(stopwords['words'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XNvbzRt4AV_6"},"source":["\n","def pre_process(data):\n","    i=0\n","    str1=' '\n","    final_string = []\n","    final_words = []\n","    all_negative_words = []\n","    s=''\n","\n","    for sentence in data:\n","        filtered_sentence = []\n","\n","        for w in sentence.split():\n","            for cleaned_word in clean_punct(w).split():\n","                if len(cleaned_word)>2:\n","                    if((cleaned_word) not in set_stop):\n","                        s = stemmer.stem_word(cleaned_word)\n","                        if len(s)>2:\n","                            final_words.append(s)\n","                            filtered_sentence.append(s)\n","                    else:\n","                        continue\n","                else:\n","                    continue\n","\n","        str1 = \" \".join(filtered_sentence)\n","        final_string.append(str1)\n","    return final_string"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JHvdUxDfAYzC"},"source":["texts = pre_process(texts)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":120},"id":"BItsM47cDwYi","executionInfo":{"status":"ok","timestamp":1606910883724,"user_tz":-360,"elapsed":1487,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"}},"outputId":"6d721d3e-22ec-4231-c982-496eaeef4f04"},"source":["texts[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'দূর ভরত হেস উঠল আপন বালিকা বধূ স্কুল পাঠানো সরলা বিবির টানাটানি সরলা বেথুন স্কুল লোরে হাউজ দুজন ধারণা স্কুল ভালো বেথুন স্কুল বাঙা পাড়া বাংলা মাধ্যম লোরেটো হাউজ সাহেব পাড় ইংরেজি স্কুল বাঙা ছাত্রীর তুলনা ফিরিঙ্গি ছাত্রী বেথুন ছাত্রী সমাজ নাম মহিলা বছর বেথুন ছাত্রী হইচ প্রবেশিকা পরীক্ষা ভালোভাব উত্তীর্ণ অবলা দাস নাম মেয়ে ডাক্তারি পড়ব কলকাতা মেডিক কলেজ ছাত্রী নেয়া অবল কেনো ডাক্তারি পারব শেষ পাঠি মাদ্রাজ মেডিক কলেজ অবল জেদ বাঙলা সরক কুড়ি টাক মাসিক বৃত্তির অনুমোদন দেশ কোথা আন্দোলন বেথুন কলেজ প্রভাব ইলবার্ট বিল সাহেব দেশী মানুষ বিদ্য বুদ্ধি প্রচ লাগল কামিনী নাম তেজস্বিনী ছাত্রীর নেতৃত্ব বেথুন বিক্ষোভ জানি সুর বাড়ুজ্য যেদিন সেদিন বেথুন ছাত্রী কালো বেধ স্কুল লোরেটো হাউজ এসব স্বদেশিয়ানা নিষিদ্ধ প্রভু যিশুর জয়গান নিয়মিত প্রার্থনা ছাত্রী ভালো ইংরেজি শেখ বিলিতি আদব কায়দা রপ্ত পাস ব্যারিস্ পত্নী হিসেব মানি বিবির বয়েস বছর সরল এগারো মামা পিসতু বোন ভাব মাঝ মাঝ তর্ক বয়েস সরল ইংরেজ শাসন সম্পর্ রাগ রাগ ভাব প্রা আবৃত্তি স্বাধীনতা হীনতা বাঁচি বাঁচি বয়েসী পরিবার নিয়ম কাকিমা মামী সরলা ছুট দুজন হাত ইকুল ভর্তি বলো আমার ভালো বউয় আড়ষ্টতা কাটেনি এমনকি বিবাহ নাম মৃণালিনী যশোর গ্রাম ভবতারিণী প্রাসাদ মানুষজন দাস দাসী দিশেহা অবস্থা রূপকথ মতন কুঁড়েঘর রাজবাড়ির বধূ রাজপুত্র মতন রূপবান স্বামী ভালো ভাব হেমেন্দ্রনাথ স্ত্রী নীপময়ীর থাক ব্যবস্থা নীপময়ী বাড়ির রীতিনীতি শেখাচ্ছ বিয় দিন কয়েকদিন উৎসব রেশ থাক কথা বাড়ি শোক ছায়া রবির বিবাহ রাত্র শিলাইদহ সারদাপ্রসাদ গাঙ্গুলির হঠাৎ হৃদরোগ মৃত্যু দিদি সৌদামিনী রবির মায় মতন তিনি সংসার কর্ত্রী সময় সত্যেন্দ্রনাথ খবর তিন পৌঁছ মূৰ্ছা যাচ্ছ বাড়ি সবা ফিসফ কথা একমাত্র বাচ্চা নিয়ম মান রবির স্ত্রী কোথা শিক্ষা গ্ৰহণ সরল তর্ক মূল্য আসল সিদ্ধান্ত নেব জ্ঞানদানন্দিনী ফেল এবার সার্কুল রোড বাড়ি ভাড়া মৃণালিনী সেখান বাড়ি লোরেটো হাউজ দূর বিবির সঙ্গ পারব গাড়ি শাড়ি টাড়ি চলব উত্তম বিলিতি কাপড় কিন স্কার্ট বানানো লাগল ব্যবস্থা অনেক মনঃপূত কাদম্বরীর সাধ স্ত্রী সঙ্গিনী মতন মেয়ে তুলব রবির আলাদা মহল নির্দিষ্ট সাজানো গোছানো সুন্দরভাব বাড়ি বেথুন স্কুল বাড়ির তিন স্কুল পাঠানোর অসুবিধ জ্ঞানদানন্দিনীর ছেল কাদম্বরীর রবির কেড় নেব জ্ঞানদানন্দিনী বাড়ি ভর্তি মানুষ গমগম রবির নিভৃ কথা সুযোগ অন্য সামন কাদম্বরী সহজভাব জিজ্ঞেস ছোট বেথুন পড়াল ভালো বাংল ইস্কুল বিব্রতভাব বলল মেজ বউঠান করলেন… নীপময়ী উপস্থিত তোর অক্ষর ইংরেজি জান কথা দেখ প্রাইমারী ইকুল বাংলা একটু আধটু শিখ ইংরেজির অক্ষরজ্ঞান লোরেটোর ফিরিঙ্গি মেয়ে পড়াশুনো পারব এক্ষুনি ইস্কুল পাঠাব দরক থাক প্রথম শিখি পড়ি দেব তোম একটু মেজ বউঠান বুঝি বলো কথা নীপাময়ী ঝংক তোর বউয় যাব পার नां রবির মুখ দেখ বোঝা জ্ঞানদানন্দিনীর এরকম প্রস্তাব তোল সাহস সঞ্চ পারব কাদম্বরী নিঃশব্দ ছায় মতন বেথুন ইস্কুল বাংল গর্ব বিরোধিতা অগ্রাহ্য এদেশ মেয়ে শিক্ষ প্রতিষ্ঠা বিদ্যাসাগর'"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"f_R0i3i1DwoM"},"source":["def load_data(num_words, sequence_length, test_size=0.25, oov_token=None):\n","    tokenizer = Tokenizer(num_words=num_words, oov_token=oov_token)\n","    num_classes=16\n","    y = to_categorical(np.asarray(labels), num_classes)\n","    tokenizer.fit_on_texts(texts)\n","    X = tokenizer.texts_to_sequences(texts)\n","    X = np.array(X)\n","    # pad sequences with 0's\n","    X = pad_sequences(X, maxlen=sequence_length)\n","    # split data to training and testing sets\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=1)\n","    data = {}\n","    data[\"X_train\"] = X_train\n","    data[\"X_test\"]= X_test\n","    data[\"y_train\"] = y_train\n","    data[\"y_test\"] = y_test\n","    data[\"tokenizer\"] = tokenizer\n","    return data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KbApwCBzDwVO"},"source":["# train_data=pd.read_csv('gdrive/My Drive/Colab Notebooks/ulm_train.csv')\n","import numpy as np\n","\n","def get_embedding_vectors(word_index, embedding_size=100):\n","    \n","    embedding_matrix = np.zeros((len(word_index) + 1, embedding_size))\n","    with open(f\"gdrive/My Drive/Colab Notebooks/bn_glove.{embedding_size}d.txt\", encoding=\"utf8\") as f:\n","        for line in tqdm(f, \"Reading GloVe\"):\n","            values = line.split()\n","            # get the word as the first word in the line\n","            word = values[0]\n","            if word in word_index:\n","                idx = word_index[word]\n","                # get the vectors as the remaining values in the line\n","                embedding_matrix[idx] = np.array(values[1:], dtype=\"float32\")\n","    return embedding_matrix"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2a0vCRMxEhfZ"},"source":["d = set()\n","for s in texts:\n","    for ss in list(s.split()):\n","        d.add(ss)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r_XIlZWTFWHB","executionInfo":{"status":"ok","timestamp":1606910902968,"user_tz":-360,"elapsed":1813,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"}},"outputId":"a98e133f-64b1-4f5c-ebc4-752311aa47f3"},"source":["len(d)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["284632"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bh6RPvx6FY4g","executionInfo":{"status":"ok","timestamp":1606910906390,"user_tz":-360,"elapsed":2011,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"}},"outputId":"c028e4ce-a9f9-49ef-cbde-6ed82ebca576"},"source":["max([len(s.split()) for s in texts]) # find the max length"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["606"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":163},"id":"VXppkk2SHxgg","executionInfo":{"status":"error","timestamp":1605981728473,"user_tz":-360,"elapsed":894,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"}},"outputId":"508ead40-d9b6-4fdb-ccd3-c67d1ace8b81"},"source":["len(data['tokenizer'].word_index)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-24-76422d5f051f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokenizer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"]}]},{"cell_type":"code","metadata":{"id":"YTzBc1JPFx6o"},"source":["MAX_SEQUENCE_LENGTH = 606    #max([len(s.split()) for s in texts]) \n","MAX_NUM_WORDS = 280432 + 1  # 'MAX_NUM_WORDS' is length of tokenizer + 1\n","vocab_size = MAX_NUM_WORDS\n","EMBEDDING_DIM = 300\n","VALIDATION_SPLIT = 0.2\n","\n","data = load_data(MAX_NUM_WORDS , MAX_SEQUENCE_LENGTH,VALIDATION_SPLIT)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kBVNWuUoKFfL","executionInfo":{"status":"ok","timestamp":1606910947175,"user_tz":-360,"elapsed":13354,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"}},"outputId":"f02c8f1a-a1d1-448d-837a-df85c65ff8fe"},"source":["embedding_matrix = get_embedding_vectors( data['tokenizer'].word_index ,EMBEDDING_DIM )"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Reading GloVe: 134256it [00:11, 11670.27it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"lcIO0YiVKFxN"},"source":["max_words = 280432 + 1\n","# max_len = 11276\n","# tok = Tokenizer(num_words=max_words)\n","# tok.fit_on_texts(X_train)\n","# sequences = tok.texts_to_sequences(X_train)\n","# sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QSgFWmdRKFkD"},"source":["from keras import backend as K\n","from keras.engine.topology import Layer\n","from keras import initializers, regularizers, constraints\n","\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU \n","from keras.layers import GRU, LSTM, BatchNormalization\n","from keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten\n","from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\n","from keras.models import Model, load_model\n","from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n","from keras import backend as K\n","from keras.engine import InputSpec, Layer\n","from keras.optimizers import Adam\n","\n","from keras.callbacks import ModelCheckpoint, TensorBoard, Callback, EarlyStopping"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qsWvHgw1KFbG"},"source":["class Attention(Layer):\n","    \"\"\"\n","    Keras Layer that implements an Attention mechanism for temporal data.\n","    Supports Masking.\n","    Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n","    # Input shape\n","        3D tensor with shape: `(samples, steps, features)`.\n","    # Output shape\n","        2D tensor with shape: `(samples, features)`.\n","    :param kwargs:\n","    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n","    The dimensions are inferred based on the output shape of the RNN.\n","    Example:\n","        model.add(LSTM(64, return_sequences=True))\n","        model.add(Attention())\n","    \"\"\"   \n","   \n","   \n","    def __init__(self, step_dim,\n","                 W_regularizer=None, b_regularizer=None,\n","                 W_constraint=None, b_constraint=None,\n","                 bias=True, **kwargs):\n","        self.supports_masking = True\n","        self.init = initializers.get('glorot_uniform')\n","\n","        self.W_regularizer = regularizers.get(W_regularizer)\n","        self.b_regularizer = regularizers.get(b_regularizer)\n","\n","        self.W_constraint = constraints.get(W_constraint)\n","        self.b_constraint = constraints.get(b_constraint)\n","\n","        self.bias = bias\n","        self.step_dim = step_dim\n","        self.features_dim = 0\n","        super(Attention, self).__init__(**kwargs)          \n","            \n","            \n","        self.features_dim = 0\n","        super(Attention, self).__init__(**kwargs)\n","\n","    def build(self, input_shape):\n","        assert len(input_shape) == 3\n","\n","        self.W = self.add_weight((input_shape[-1],),\n","                                 initializer=self.init,\n","                                 name='{}_W'.format(self.name),\n","                                 regularizer=self.W_regularizer,\n","                                 constraint=self.W_constraint)\n","        self.features_dim = input_shape[-1]\n","\n","        if self.bias:\n","            self.b = self.add_weight((input_shape[1],),\n","                                     initializer='zero',\n","                                     name='{}_b'.format(self.name),\n","                                     regularizer=self.b_regularizer,\n","                                     constraint=self.b_constraint)\n","        else:\n","            self.b = None\n","        self.built = True\n","    \n","    def compute_mask(self, input, input_mask=None):\n","        return None\n","\n","    def call(self, x, mask=None):\n","        features_dim = self.features_dim\n","        step_dim = self.step_dim\n","        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n","                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n","        if self.bias:\n","            eij += self.b\n","        eij = K.tanh(eij)\n","        a = K.exp(eij)\n","        if mask is not None:\n","            a *= K.cast(mask, K.floatx())\n","        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n","        a = K.expand_dims(a)\n","        weighted_input = x * a\n","        return K.sum(weighted_input, axis=1)\n","\n","    def compute_output_shape(self, input_shape):\n","        return input_shape[0],  self.features_dim"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kFnVIiJqKfa1"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"g3npxFC_KlWH"},"source":["max_len = 606 (as usual), bs=64"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"PKpn49hPKFZj","executionInfo":{"status":"ok","timestamp":1606910966628,"user_tz":-360,"elapsed":4732,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"}},"outputId":"8815ea67-6246-47ab-8a79-f4dc2323cddc"},"source":["#max_features = 30000\n","max_features = 280432 + 1\n","max_len = 606\n","def build_model(max_len, max_features, EMBEDDING_DIM, embedding_matrix):\n","    input_words = Input((max_len, ))\n","    x_words = Embedding(max_features,\n","                        EMBEDDING_DIM,\n","                        weights=[embedding_matrix],\n","                        mask_zero=True,\n","                        trainable=False)(input_words)\n","    x_words = SpatialDropout1D(0.2)(x_words)\n","    x_words = Bidirectional(LSTM(128, return_sequences=True))(x_words)\n","    x_words = Bidirectional(LSTM(128, return_sequences=True))(x_words)\n","    \n","    x = Attention(max_len)(x_words)\n","    #x = GlobalMaxPooling1D()(x)\n","    #x = GlobalAveragePooling1D()(x)\n","    x = Dropout(0.2)(x)\n","    x = Dense(64, activation='relu')(x)\n","    x = Dropout(0.2)(x)\n","    pred = Dense(16, activation='softmax')(x)\n","\n","    model = Model(inputs=input_words, outputs=pred)\n","    return model\n","\n","model = build_model(max_len, max_features, EMBEDDING_DIM, embedding_matrix)\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","model.summary()\n","'''\n","MAX_SEQUENCE_LENGTH = 11276    #max([len(s.split()) for s in texts]) \n","MAX_NUM_WORDS = 109803 + 1\n","vocab_size = MAX_NUM_WORDS\n","EMBEDDING_DIM = 300\n","VALIDATION_SPLIT = 0.2\n","\n","data = load_data(MAX_NUM_WORDS , MAX_SEQUENCE_LENGTH,VALIDATION_SPLIT)\n","'''"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2974: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n","\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_1 (InputLayer)         (None, 606)               0         \n","_________________________________________________________________\n","embedding_1 (Embedding)      (None, 606, 300)          84129900  \n","_________________________________________________________________\n","spatial_dropout1d_1 (Spatial (None, 606, 300)          0         \n","_________________________________________________________________\n","bidirectional_1 (Bidirection (None, 606, 256)          439296    \n","_________________________________________________________________\n","bidirectional_2 (Bidirection (None, 606, 256)          394240    \n","_________________________________________________________________\n","attention_2 (Attention)      (None, 256)               862       \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 256)               0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 64)                16448     \n","_________________________________________________________________\n","dropout_2 (Dropout)          (None, 64)                0         \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 16)                1040      \n","=================================================================\n","Total params: 84,981,786\n","Trainable params: 851,886\n","Non-trainable params: 84,129,900\n","_________________________________________________________________\n"],"name":"stdout"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\nMAX_SEQUENCE_LENGTH = 11276    #max([len(s.split()) for s in texts]) \\nMAX_NUM_WORDS = 109803 + 1\\nvocab_size = MAX_NUM_WORDS\\nEMBEDDING_DIM = 300\\nVALIDATION_SPLIT = 0.2\\n\\ndata = load_data(MAX_NUM_WORDS , MAX_SEQUENCE_LENGTH,VALIDATION_SPLIT)\\n'"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"markdown","metadata":{"id":"cIPdMxOrcbjy"},"source":["epochs = 40 with loss curve and f1 score"]},{"cell_type":"code","metadata":{"id":"GRnDUvhsQsRW"},"source":["history = model.fit(data['X_train'], data['y_train'], validation_data=(data['X_test'], data['y_test']), epochs=5, verbose=2, batch_size=64)\n","model.save(\"trainedmodel_5Epoch.h5\") # saving the model\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9xxC8kjlWCLg"},"source":["from keras.models import load_model\n","model = load_model('trainedmodel_5Epoch.h5')# loading model trained for 50 Epochs\n","hstry = model.fit(data['X_train'], data['y_train'], validation_data=(data['X_test'], data['y_test']), epochs=5, verbose=2)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wB7q-EjuWCCx"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v3WRIbbBcWYl","outputId":"b79cca46-be54-4548-c1ce-875718343823"},"source":["history = model.fit(data['X_train'], data['y_train'], validation_data=(data['X_test'], data['y_test']), epochs=20, verbose=2, batch_size=64)\n","\n","def history2(history):\n","    # list all data in history\n","    print(history.history.keys())\n","    # summarize history for accuracy\n","    plt.plot(history.history['acc'])\n","    plt.plot(history.history['val_acc'])\n","    plt.title('model accuracy')\n","    plt.ylabel('accuracy')\n","    plt.xlabel('epoch')\n","    plt.legend(['train', 'test'], loc='upper left')\n","    plt.show()\n","    # summarize history for loss\n","    plt.plot(history.history['loss'])\n","    plt.plot(history.history['val_loss'])\n","    plt.title('model loss')\n","    plt.ylabel('loss')\n","    plt.xlabel('epoch')\n","    plt.legend(['train', 'test'], loc='upper left')\n","    plt.show()\n","\n","history2(history)\n","\n","import seaborn as sns\n","from sklearn.metrics import confusion_matrix\n","import matplotlib.pyplot as plt\n","def confussion_mat(model):\n","    # Predict the values from the validation dataset\n","    Y_pred = model.predict(data['X_test'])\n","    # Convert predictions classes to one hot vectors \n","    Y_pred_classes = np.argmax(Y_pred,axis = 1) \n","    # Convert validation observations to one hot vectors\n","    Y_true = np.argmax(data['y_test'],axis = 1) \n","    # compute the confusion matrix\n","    confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n","    # plot the confusion matrix\n","    f,ax = plt.subplots(figsize=(15, 15))\n","    sns.heatmap(confusion_mtx, annot=True, linewidths=0.01,cmap=\"Greens\",linecolor=\"gray\", fmt= '.1f',ax=ax)\n","    plt.xlabel(\"Predicted Label\")\n","    plt.ylabel(\"True Label\")\n","    plt.title(\"Confusion Matrix\")\n","    plt.show()\n","\n","confussion_mat(model)\n","\n","from sklearn.datasets import make_classification\n","from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n","    \n","Y_pred = model.predict(data['X_test'])\n","# Convert predictions classes to one hot vectors \n","Y_pred_classes = np.argmax(Y_pred,axis = 1) \n","# Convert validation observations to one hot vectors\n","Y_true = np.argmax(data['y_test'],axis = 1) \n","# compute the confusion matrix\n","print(\"f1-score:\")\n","print(f1_score(Y_true, Y_pred_classes, average=\"macro\"))\n","print(\"precision_score:\")\n","print(precision_score(Y_true, Y_pred_classes, average=\"macro\"))\n","print(\"recall_score:\")\n","print(recall_score(Y_true, Y_pred_classes, average=\"macro\")) \n","\n","# print(\"F1-score: {:.1%}\".format(f1_score(Y_true, Y_pred_classes)))\n","print(classification_report(Y_true, Y_pred_classes))\n","\n","model.save(\"trainedmodel_20Epoch.h5\") # saving the model\n","#model = load_model('trainedmodel_11Epoch.h5')# loading model trained for 50 Epochs\n","#history = model.fit(data['X_train'], data['y_train'], validation_data=(data['X_test'], data['y_test']), epochs=4, verbose=2)\n","\n","hist_df = pd.DataFrame(history.history) \n","hist_json_file = 'history.json' \n","with open(hist_json_file, mode='w') as f:\n","    hist_df.to_json(f)\n","\n","hist_csv_file = 'history.csv'\n","with open(hist_csv_file, mode='w') as f:\n","    hist_df.to_csv(f)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n","\n","Train on 14372 samples, validate on 3594 samples\n","Epoch 1/20\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hd7rdT4nKtwI"},"source":["import tensorflow as tf\n","callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"taPPDEIspwkN"},"source":["# load the save model and history and again run on 50 epochs\n","'''\n","from keras.models import load_model\n","model = load_model('trainedmodel_50Epoch.h5')# loading model trained for 50 Epochs\n","\n","hstry = model.fit_generator(......) # training the model for another 50 Epochs\n","\n","model.save(\"trainedmodel_50Epoch.h5\") # saving the model \n","\n","with open('trainHistoryOld', 'wb') as handle: # saving the history of the model trained for another 50 Epochs\n","    dump(hstry.history, handle)\n","\n","from pickle import load\n","import matplotlib.pyplot as plt\n","\n","with open('trainHistoryOld', 'rb') as handle: # loading old history \n","    oldhstry = load(handle)\n","\n","oldhstry['loss'].extend(hstry['loss'])\n","oldhstry['acc'].extend(hstry['acc'])\n","oldhstry['val_loss'].extend(hstry['val_loss'])\n","oldhstry['val_acc'].extend(hstry['val_acc'])\n","\n","# Plotting the Accuracy vs Epoch Graph\n","plt.plot(oldhstry['acc'])\n","plt.plot(oldhstry['val_acc'])\n","plt.title('model accuracy')\n","plt.ylabel('accuracy')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'test'], loc='upper left')\n","plt.show()\n","\n","# Plotting the Loss vs Epoch Graphs\n","plt.plot(oldhstry['loss'])\n","plt.plot(oldhstry['val_loss'])\n","plt.title('model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'test'], loc='upper left')\n","plt.show()\n","\n","'''"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yk8XCaClJB4g"},"source":["import tensorflow as tf\n","callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n"],"execution_count":null,"outputs":[]}]}