{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ATTENTION with LSTM main 12 news.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN9A8aZJNdXHTeS+gZ6pBWr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"44FcGq1zplEd","executionInfo":{"status":"ok","timestamp":1606077455521,"user_tz":-360,"elapsed":51506,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"}},"outputId":"dd6a3062-5730-4d12-9a4d-42affe257330"},"source":["!pip install Keras==2.2.4\n","!pip install tensorflow==1.14.0\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting Keras==2.2.4\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/10/aa32dad071ce52b5502266b5c659451cfd6ffcbf14e6c8c4f16c0ff5aaab/Keras-2.2.4-py2.py3-none-any.whl (312kB)\n","\r\u001b[K     |█                               | 10kB 18.3MB/s eta 0:00:01\r\u001b[K     |██                              | 20kB 23.0MB/s eta 0:00:01\r\u001b[K     |███▏                            | 30kB 17.1MB/s eta 0:00:01\r\u001b[K     |████▏                           | 40kB 11.8MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 51kB 7.7MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 61kB 8.1MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 71kB 8.0MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 81kB 8.8MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 92kB 9.4MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 102kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 112kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 122kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 133kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 143kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 153kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 163kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 174kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 184kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 194kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 204kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 215kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 225kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 235kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 245kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 256kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 266kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 276kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 286kB 9.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 296kB 9.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 307kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 317kB 9.6MB/s \n","\u001b[?25hRequirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras==2.2.4) (1.4.1)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras==2.2.4) (3.13)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras==2.2.4) (1.15.0)\n","Collecting keras-applications>=1.0.6\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n","\u001b[K     |████████████████████████████████| 51kB 4.4MB/s \n","\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras==2.2.4) (2.10.0)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras==2.2.4) (1.1.2)\n","Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from Keras==2.2.4) (1.18.5)\n","Installing collected packages: keras-applications, Keras\n","  Found existing installation: Keras 2.4.3\n","    Uninstalling Keras-2.4.3:\n","      Successfully uninstalled Keras-2.4.3\n","Successfully installed Keras-2.2.4 keras-applications-1.0.8\n","Collecting tensorflow==1.14.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/f0/96fb2e0412ae9692dbf400e5b04432885f677ad6241c088ccc5fe7724d69/tensorflow-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (109.2MB)\n","\u001b[K     |████████████████████████████████| 109.2MB 93kB/s \n","\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.15.0)\n","Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.2.0)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.12.1)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.1.0)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.8.1)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.10.0)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.33.2)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.35.1)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.0.8)\n","Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.18.5)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (3.12.4)\n","Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\n","\u001b[K     |████████████████████████████████| 491kB 47.6MB/s \n","\u001b[?25hRequirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.3.3)\n","Collecting tensorboard<1.15.0,>=1.14.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl (3.1MB)\n","\u001b[K     |████████████████████████████████| 3.2MB 50.6MB/s \n","\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.1.2)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.14.0) (2.10.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.14.0) (50.3.2)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (1.0.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.3.3)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (2.0.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.4.0)\n","Installing collected packages: tensorflow-estimator, tensorboard, tensorflow\n","  Found existing installation: tensorflow-estimator 2.3.0\n","    Uninstalling tensorflow-estimator-2.3.0:\n","      Successfully uninstalled tensorflow-estimator-2.3.0\n","  Found existing installation: tensorboard 2.3.0\n","    Uninstalling tensorboard-2.3.0:\n","      Successfully uninstalled tensorboard-2.3.0\n","  Found existing installation: tensorflow 2.3.0\n","    Uninstalling tensorflow-2.3.0:\n","      Successfully uninstalled tensorflow-2.3.0\n","Successfully installed tensorboard-1.14.0 tensorflow-1.14.0 tensorflow-estimator-1.14.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"d7_3CM0ipydR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606077499313,"user_tz":-360,"elapsed":2811,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"}},"outputId":"3fc9f90b-bbd4-467a-e94a-2bb9a9273b6b"},"source":["import os\n","import re\n","import pickle\n","from tqdm import tqdm\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","from tensorflow.keras.layers import Dense, Dropout, Embedding, Bidirectional, LSTM\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.callbacks import TensorBoard\n","from sklearn.model_selection import train_test_split\n","# from keras.layers import Embedding\n","from  keras . utils  import  to_categorical\n","from tensorflow.keras.layers import Dense, Input, GlobalMaxPooling1D, Permute\n","from tensorflow.keras.layers import Conv1D,Conv2D, MaxPooling1D, Embedding, Flatten\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.initializers import Constant\n","from tensorflow.keras.layers import Embedding\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from keras.models import Model\n","from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n","from keras.optimizers import RMSprop\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing import sequence\n","from keras.utils import to_categorical\n","from keras.callbacks import EarlyStopping\n","%matplotlib inline\n","\n","\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from keras.models import Model\n","from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n","from keras.optimizers import RMSprop\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing import sequence\n","from keras.utils import to_categorical\n","from keras.callbacks import EarlyStopping\n","%matplotlib inline"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n","Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mqif4oj4rbxi","executionInfo":{"status":"ok","timestamp":1606077552454,"user_tz":-360,"elapsed":41118,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"}},"outputId":"f4dedb26-6448-4e1d-97c4-3011e4450bb5"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"70iGHIp2rfZp"},"source":["import pandas as pd \n","df_train=pd.read_csv('gdrive/My Drive/Thesis Data/full doc csv/full_doc_train.csv')\n","df_test=pd.read_csv('gdrive/My Drive/Thesis Data/full doc csv/full_doc_test.csv')\n","total_data = pd.concat([df_train,df_test])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_XMG6RHHrzDq","executionInfo":{"status":"ok","timestamp":1606077608844,"user_tz":-360,"elapsed":967,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"}},"outputId":"4c6431f5-4b65-4d72-8f68-52ab4dc0aa22"},"source":["print(len(total_data))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["95855\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6MLP8z_Qr2-K","executionInfo":{"status":"ok","timestamp":1606077624098,"user_tz":-360,"elapsed":1880,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"}},"outputId":"0eb468cc-5559-4a2c-e471-abae51be56e2"},"source":["!git clone -l -s https://github.com/banglakit/bengali-stemmer.git"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Cloning into 'bengali-stemmer'...\n","warning: --local is ignored\n","remote: Enumerating objects: 25, done.\u001b[K\n","remote: Counting objects: 100% (25/25), done.\u001b[K\n","remote: Compressing objects: 100% (17/17), done.\u001b[K\n","remote: Total 94 (delta 5), reused 16 (delta 4), pack-reused 69\u001b[K\n","Unpacking objects: 100% (94/94), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3tiPwV24r6eS","executionInfo":{"status":"ok","timestamp":1606077642835,"user_tz":-360,"elapsed":4609,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"}},"outputId":"7bee9e12-db1e-4928-d992-265a63ee54a9"},"source":["!pip install git+https://github.com/banglakit/bengali-stemmer.git    "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting git+https://github.com/banglakit/bengali-stemmer.git\n","  Cloning https://github.com/banglakit/bengali-stemmer.git to /tmp/pip-req-build-qf4f4bz1\n","  Running command git clone -q https://github.com/banglakit/bengali-stemmer.git /tmp/pip-req-build-qf4f4bz1\n","Building wheels for collected packages: bengali-stemmer\n","  Building wheel for bengali-stemmer (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for bengali-stemmer: filename=bengali_stemmer-0.0.1-py2.py3-none-any.whl size=6393 sha256=a2d867cb781a4e39437b9aa2d7282a48f4e2ca84c8e65c68c4c68b5999e1297d\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-di4jq0l1/wheels/a1/ad/a1/4ba354059b17c00600a14e13a504e7bdb49f20f2f4e2f3639c\n","Successfully built bengali-stemmer\n","Installing collected packages: bengali-stemmer\n","Successfully installed bengali-stemmer-0.0.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"532-fUN6r-Ya","executionInfo":{"status":"ok","timestamp":1606077652894,"user_tz":-360,"elapsed":4548,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"}},"outputId":"241851a6-292f-4ba0-f3bf-e343645e954b"},"source":["!pip install git+https://github.com/banglakit/bengali-stemmer.git    "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting git+https://github.com/banglakit/bengali-stemmer.git\n","  Cloning https://github.com/banglakit/bengali-stemmer.git to /tmp/pip-req-build-g_p2y8nm\n","  Running command git clone -q https://github.com/banglakit/bengali-stemmer.git /tmp/pip-req-build-g_p2y8nm\n","Requirement already satisfied (use --upgrade to upgrade): bengali-stemmer==0.0.1 from git+https://github.com/banglakit/bengali-stemmer.git in /usr/local/lib/python3.6/dist-packages\n","Building wheels for collected packages: bengali-stemmer\n","  Building wheel for bengali-stemmer (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for bengali-stemmer: filename=bengali_stemmer-0.0.1-py2.py3-none-any.whl size=6393 sha256=29078b35adaac382bd8c9d3bc914f242663d11b05c57db70266194be7fe87867\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-ltp3wsh5/wheels/a1/ad/a1/4ba354059b17c00600a14e13a504e7bdb49f20f2f4e2f3639c\n","Successfully built bengali-stemmer\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"Z3wAGO6nsA2a","executionInfo":{"status":"ok","timestamp":1606077666740,"user_tz":-360,"elapsed":973,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"}},"outputId":"f1b4dc53-068f-40eb-9c55-e5fd0dfe5b22"},"source":["from bengali_stemmer.rafikamal2014 import RafiStemmer\n","stemmer = RafiStemmer()\n","stemmer.stem_word('বাংলায়')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'বাংলা'"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"M8FpmXWSsFGq"},"source":["macronum=sorted(set(total_data['label']))\n","macro_to_id = dict((note, number) for number, note in enumerate(macronum))\n","\n","def fun(i):\n","    return macro_to_id[i]\n","\n","total_data['label']=total_data['label'].apply(fun)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MzbwN_qfsKS_"},"source":["texts = list(total_data['text'])\n","labels = list(total_data['label'])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M9YMLz_7sc1q"},"source":["def clean_punct(sentence):\n","    cleaned = re.sub(r'[?|!|\\'|\"|#|।|’|‘]', r'', sentence)\n","    cleaned1 = re.sub(r'[.|,|(|)|\\|/]', r'', cleaned)\n","    cleaned = re.sub(r'[০|১|২|৩|৪|৫|৬|৭|৮|৯]', r'', cleaned1)\n","    cleaned1 = re.sub(r'[-|=]', r' ', cleaned)\n","    return cleaned1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XBmgtew0sgAJ"},"source":["stopwords = pd.read_csv('gdrive/My Drive/Colab Notebooks/Stopwords.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tm4FKDe4sjoR"},"source":["set_stop = set(stopwords['words'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3l_we_TQsmrZ"},"source":["\n","def pre_process(data):\n","    i=0\n","    str1=' '\n","    final_string = []\n","    final_words = []\n","    all_negative_words = []\n","    s=''\n","\n","    for sentence in data:\n","        filtered_sentence = []\n","\n","        for w in sentence.split():\n","            for cleaned_word in clean_punct(w).split():\n","                if len(cleaned_word)>2:\n","                    if((cleaned_word) not in set_stop):\n","                        s = stemmer.stem_word(cleaned_word)\n","                        if len(s)>2:\n","                            final_words.append(s)\n","                            filtered_sentence.append(s)\n","                    else:\n","                        continue\n","                else:\n","                    continue\n","\n","        str1 = \" \".join(filtered_sentence)\n","        final_string.append(str1)\n","    return final_string"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tdExGgL7spjx"},"source":["texts = pre_process(texts)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":120},"id":"M5SucDyxttDB","executionInfo":{"status":"ok","timestamp":1606078113547,"user_tz":-360,"elapsed":1036,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"}},"outputId":"e4ea0199-1c66-4f26-c793-ecf259cc7ccb"},"source":["texts[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'উচ্চ মাধ্যমিক পরীক্ষ মঙ্গলব বিবৃতি মন্ত্রী “আমি হরতাল অবরোধ সন্ত্রাসী কার্যক্রম পরিচালনাকারী জোট আহ্বান জানা পরীক্ষার্থী শান্তিপূর্ণভাব পরীক্ষা দয়া হটকারী ঘটনা ঘটাব “আমি ভাষা দেশ স্থান একজন পরীক্ষার্থীর ক্ষতি দায়দায়িত্ব আপনা বহন মানুষ আপনা ক্ষমা না” জানুয়ারি দেশ টানা অবরোধ চালি আসা বিএনপি জোট ফেব্রুয়ারি মার্চ বেশিরভাগ প্রতিদিন হরতাল বিএনপি জোট অবরোধ হরতাল চলতি এসএসসি সমমান অর্থাৎ দিন পরীক্ষা পেছা বাধ্য শিক্ষা মন্ত্রণাল পরীক্ষা দিন শুক্র শনিব ধরন রাজনৈতিক কর্মসূচির কারণ এইচএসসি পরীক্ষা পেছানো রেখ শিক্ষামন্ত্রী ফাইল বিবৃতি নাহিদ “সংকট এসএসসি পরীক্ষা শেষ এইচএসসি পরীক্ষা জাতির দুর্ভাগ্য রাজনৈতিক জোট বিবেকবর্জিত অব্যাহত হরতাল অবরোধ কারণ এসএসসির রুটিন পরীক্ষা সম্ভব “পরীক্ষার্থী অভিভাবক শিক্ষকসহ শ্রেণি পেশ মানুষ মতামত পরামর্শ সম্মান দেখি পরিস্থিতি রুটিনমাফিক পরীক্ষা সিদ্ধান্ত গ্রহণ করে ব্যত্য না” পরীক্ষার্থী যাতায়াত নির্বিঘ্ন আইনশৃঙ্খলা রক্ষাবাহিনীর সর্বাত্মক নজরদারি আশ্বস্ত শিক্ষামন্ত্রী “ইতিবাচক দৃষ্টিভঙ্গিসম্পন্ন রাজনৈতিক নেতা কর্মী সামাজিক সাংস্কৃতিক সংগঠন সদস্য জনগণ তোমা পাশ তোম নিশ্চিন্ত পরীক্ষা দেবে” প্রশ্ন ফাঁস গুজব শিক্ষার্থী অভিভাবক বিভ্রান্ত আহ্বান পরীক্ষার্থী লেখাপড়া পরামর্শ শিক্ষামন্ত্রী অবরোধ এইচএসসি সমমান পরীক্ষা বসছ লাখ শিক্ষার্থী এপ্রিল জুন এইচএসসি সমমান তত্ত্বী বিষয় পরীক্ষা ব্যবহারিক পরীক্ষা জুন জুন'"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"id":"TLUvzZmAsxBB"},"source":["def load_data(num_words, sequence_length, test_size=0.25, oov_token=None):\n","    tokenizer = Tokenizer(num_words=num_words, oov_token=oov_token)\n","    num_classes = 12\n","    y = to_categorical(np.asarray(labels), num_classes)\n","    tokenizer.fit_on_texts(texts)\n","    X = tokenizer.texts_to_sequences(texts)\n","    X = np.array(X)\n","    # pad sequences with 0's\n","    X = pad_sequences(X, maxlen=sequence_length)\n","    # split data to training and testing sets\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=1)\n","    data = {}\n","    data[\"X_train\"] = X_train\n","    data[\"X_test\"]= X_test\n","    data[\"y_train\"] = y_train\n","    data[\"y_test\"] = y_test\n","    data[\"tokenizer\"] = tokenizer\n","    return data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TLnTy6J9sxCe"},"source":["# train_data=pd.read_csv('gdrive/My Drive/Colab Notebooks/ulm_train.csv')\n","import numpy as np\n","\n","def get_embedding_vectors(word_index, embedding_size=100):\n","    \n","    embedding_matrix = np.zeros((len(word_index) + 1, embedding_size))\n","    with open(f\"gdrive/My Drive/Colab Notebooks/bn_glove.{embedding_size}d.txt\", encoding=\"utf8\") as f:\n","        for line in tqdm(f, \"Reading GloVe\"):\n","            values = line.split()\n","            # get the word as the first word in the line\n","            word = values[0]\n","            if word in word_index:\n","                idx = word_index[word]\n","                # get the vectors as the remaining values in the line\n","                embedding_matrix[idx] = np.array(values[1:], dtype=\"float32\")\n","    return embedding_matrix"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3jfYY-QNsxJI"},"source":["d = set()\n","for s in texts:\n","    for ss in list(s.split()):\n","        d.add(ss)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IrVP6pJXsxFs","executionInfo":{"status":"ok","timestamp":1606078132975,"user_tz":-360,"elapsed":1014,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"}},"outputId":"871a435a-1d92-461c-a008-9a8ee26b941c"},"source":["len(d) # here is the total number of words in corpus, but the number of tokenizer is small then that(ther are some stopwords which is removed)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["368309"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I4VFXEJ1tDUA","executionInfo":{"status":"ok","timestamp":1606078150647,"user_tz":-360,"elapsed":2478,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"}},"outputId":"9480e7ba-84a9-4163-a59e-222c336b0f56"},"source":["max([len(s.split()) for s in texts]) # find the max length"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["7779"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"code","metadata":{"id":"qFIhhORQtDbG"},"source":["# MAX_SEQUENCE_LENGTH = 7779    #max([len(s.split()) for s in texts]) \n","max_len = 2500\n","MAX_NUM_WORDS = 355559 + 1 # you have to use word same as tokenizer length, and add extra 1(for 0th index case)\n","vocab_size = MAX_NUM_WORDS\n","EMBEDDING_DIM = 300\n","VALIDATION_SPLIT = 0.2\n","\n","data = load_data(MAX_NUM_WORDS , max_len,VALIDATION_SPLIT)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9oEqXcj-tMNn","executionInfo":{"status":"ok","timestamp":1606078240486,"user_tz":-360,"elapsed":986,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"}},"outputId":"745a4bcd-687e-4aa4-c717-9233324eb5cf"},"source":["len(data['tokenizer'].word_index) # after running previous cell of code we run this cell,we call the 'load_data' function before we see the len of data['tokenizer']"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["355559"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I9An2wfTtMJD","executionInfo":{"status":"ok","timestamp":1606078359548,"user_tz":-360,"elapsed":10175,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"}},"outputId":"eea62530-4ada-4c86-c8df-6aba255aa5b5"},"source":["embedding_matrix = get_embedding_vectors( data['tokenizer'].word_index ,EMBEDDING_DIM )"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Reading GloVe: 134256it [00:09, 14667.20it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"sPRrnVfDtDX3"},"source":["max_words = 355559 + 1\n","# max_len = 7779 \n","# tok = Tokenizer(num_words=max_words)\n","# tok.fit_on_texts(X_train)\n","# sequences = tok.texts_to_sequences(X_train)\n","# sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OMeP06A5swZq"},"source":["from keras import backend as K\n","from keras.engine.topology import Layer\n","from keras import initializers, regularizers, constraints\n","\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU \n","from keras.layers import GRU, LSTM, BatchNormalization\n","from keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten\n","from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\n","from keras.models import Model, load_model\n","from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n","from keras import backend as K\n","from keras.engine import InputSpec, Layer\n","from keras.optimizers import Adam\n","\n","from keras.callbacks import ModelCheckpoint, TensorBoard, Callback, EarlyStopping"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_SWKzZuQta-A"},"source":["class Attention(Layer):\n","    \"\"\"\n","    Keras Layer that implements an Attention mechanism for temporal data.\n","    Supports Masking.\n","    Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n","    # Input shape\n","        3D tensor with shape: `(samples, steps, features)`.\n","    # Output shape\n","        2D tensor with shape: `(samples, features)`.\n","    :param kwargs:\n","    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n","    The dimensions are inferred based on the output shape of the RNN.\n","    Example:\n","        model.add(LSTM(64, return_sequences=True))\n","        model.add(Attention())\n","    \"\"\"   \n","   \n","   \n","    def __init__(self, step_dim,\n","                 W_regularizer=None, b_regularizer=None,\n","                 W_constraint=None, b_constraint=None,\n","                 bias=True, **kwargs):\n","        self.supports_masking = True\n","        self.init = initializers.get('glorot_uniform')\n","\n","        self.W_regularizer = regularizers.get(W_regularizer)\n","        self.b_regularizer = regularizers.get(b_regularizer)\n","\n","        self.W_constraint = constraints.get(W_constraint)\n","        self.b_constraint = constraints.get(b_constraint)\n","\n","        self.bias = bias\n","        self.step_dim = step_dim\n","        self.features_dim = 0\n","        super(Attention, self).__init__(**kwargs)          \n","            \n","            \n","        self.features_dim = 0\n","        super(Attention, self).__init__(**kwargs)\n","\n","    def build(self, input_shape):\n","        assert len(input_shape) == 3\n","\n","        self.W = self.add_weight((input_shape[-1],),\n","                                 initializer=self.init,\n","                                 name='{}_W'.format(self.name),\n","                                 regularizer=self.W_regularizer,\n","                                 constraint=self.W_constraint)\n","        self.features_dim = input_shape[-1]\n","\n","        if self.bias:\n","            self.b = self.add_weight((input_shape[1],),\n","                                     initializer='zero',\n","                                     name='{}_b'.format(self.name),\n","                                     regularizer=self.b_regularizer,\n","                                     constraint=self.b_constraint)\n","        else:\n","            self.b = None\n","        self.built = True\n","    \n","    def compute_mask(self, input, input_mask=None):\n","        return None\n","\n","    def call(self, x, mask=None):\n","        features_dim = self.features_dim\n","        step_dim = self.step_dim\n","        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n","                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n","        if self.bias:\n","            eij += self.b\n","        eij = K.tanh(eij)\n","        a = K.exp(eij)\n","        if mask is not None:\n","            a *= K.cast(mask, K.floatx())\n","        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n","        a = K.expand_dims(a)\n","        weighted_input = x * a\n","        return K.sum(weighted_input, axis=1)\n","\n","    def compute_output_shape(self, input_shape):\n","        return input_shape[0],  self.features_dim"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":511},"id":"rmYJLJDUtbGu","executionInfo":{"status":"ok","timestamp":1606078448222,"user_tz":-360,"elapsed":3747,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"}},"outputId":"de7e2683-d7c8-47fe-d948-aca61c28bc23"},"source":["max_features = 355559 + 1\n","def build_model(max_len, max_features, EMBEDDING_DIM, embedding_matrix):\n","    input_words = Input((max_len, ))\n","    x_words = Embedding(max_features,\n","                        EMBEDDING_DIM,\n","                        weights=[embedding_matrix],\n","                        mask_zero=True,\n","                        trainable=False)(input_words)\n","    x_words = SpatialDropout1D(0.2)(x_words)\n","    x_words = Bidirectional(LSTM(128, return_sequences=True))(x_words)\n","    x_words = Bidirectional(LSTM(128, return_sequences=True))(x_words)\n","    \n","    x = Attention(max_len)(x_words)\n","    #x = GlobalMaxPooling1D()(x)\n","    #x = GlobalAveragePooling1D()(x)\n","    x = Dropout(0.2)(x)\n","    x = Dense(64, activation='relu')(x)\n","    x = Dropout(0.2)(x)\n","    pred = Dense(12, activation='softmax')(x)\n","\n","    model = Model(inputs=input_words, outputs=pred)\n","    return model\n","\n","model = build_model(max_len, max_features, EMBEDDING_DIM, embedding_matrix)\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","model.summary()\n","'''\n","MAX_SEQUENCE_LENGTH = 11276    #max([len(s.split()) for s in texts]) \n","MAX_NUM_WORDS = 109803 + 1\n","vocab_size = MAX_NUM_WORDS\n","EMBEDDING_DIM = 300\n","VALIDATION_SPLIT = 0.2\n","\n","data = load_data(MAX_NUM_WORDS , MAX_SEQUENCE_LENGTH,VALIDATION_SPLIT)\n","'''"],"execution_count":null,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_2 (InputLayer)         (None, 2500)              0         \n","_________________________________________________________________\n","embedding_2 (Embedding)      (None, 2500, 300)         106668000 \n","_________________________________________________________________\n","spatial_dropout1d_2 (Spatial (None, 2500, 300)         0         \n","_________________________________________________________________\n","bidirectional_3 (Bidirection (None, 2500, 256)         439296    \n","_________________________________________________________________\n","bidirectional_4 (Bidirection (None, 2500, 256)         394240    \n","_________________________________________________________________\n","attention_4 (Attention)      (None, 256)               2756      \n","_________________________________________________________________\n","dropout_3 (Dropout)          (None, 256)               0         \n","_________________________________________________________________\n","dense_3 (Dense)              (None, 64)                16448     \n","_________________________________________________________________\n","dropout_4 (Dropout)          (None, 64)                0         \n","_________________________________________________________________\n","dense_4 (Dense)              (None, 12)                780       \n","=================================================================\n","Total params: 107,521,520\n","Trainable params: 853,520\n","Non-trainable params: 106,668,000\n","_________________________________________________________________\n"],"name":"stdout"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\nMAX_SEQUENCE_LENGTH = 11276    #max([len(s.split()) for s in texts]) \\nMAX_NUM_WORDS = 109803 + 1\\nvocab_size = MAX_NUM_WORDS\\nEMBEDDING_DIM = 300\\nVALIDATION_SPLIT = 0.2\\n\\ndata = load_data(MAX_NUM_WORDS , MAX_SEQUENCE_LENGTH,VALIDATION_SPLIT)\\n'"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"code","metadata":{"id":"as0Hijr3tbDg"},"source":["import tensorflow as tf\n","callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"82czYqQata7D"},"source":["# use only 10 epochs\n","history = model.fit(data['X_train'], data['y_train'], validation_data=(data['X_test'], data['y_test']), epochs=10, verbose=2, batch_size=64, callbacks=[callback])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jWdzQaIRsvyC"},"source":[""],"execution_count":null,"outputs":[]}]}