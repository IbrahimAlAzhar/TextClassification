{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5830,"status":"ok","timestamp":1606163531839,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"},"user_tz":-360},"id":"Jma2a0x32pjK","outputId":"fa467c14-ecec-4809-9230-10b2d38c4943"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: Keras==2.2.4 in /usr/local/lib/python3.6/dist-packages (2.2.4)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras==2.2.4) (2.10.0)\n","Requirement already satisfied: six\u003e=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras==2.2.4) (1.15.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras==2.2.4) (3.13)\n","Requirement already satisfied: numpy\u003e=1.9.1 in /usr/local/lib/python3.6/dist-packages (from Keras==2.2.4) (1.18.5)\n","Requirement already satisfied: keras-preprocessing\u003e=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras==2.2.4) (1.1.2)\n","Requirement already satisfied: keras-applications\u003e=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras==2.2.4) (1.0.8)\n","Requirement already satisfied: scipy\u003e=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras==2.2.4) (1.4.1)\n","Requirement already satisfied: tensorflow==1.14.0 in /usr/local/lib/python3.6/dist-packages (1.14.0)\n","Requirement already satisfied: tensorflow-estimator\u003c1.15.0rc0,\u003e=1.14.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.14.0)\n","Requirement already satisfied: six\u003e=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.15.0)\n","Requirement already satisfied: grpcio\u003e=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.33.2)\n","Requirement already satisfied: protobuf\u003e=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (3.12.4)\n","Requirement already satisfied: numpy\u003c2.0,\u003e=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.18.5)\n","Requirement already satisfied: wheel\u003e=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.35.1)\n","Requirement already satisfied: gast\u003e=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.3.3)\n","Requirement already satisfied: keras-preprocessing\u003e=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.1.2)\n","Requirement already satisfied: termcolor\u003e=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.1.0)\n","Requirement already satisfied: astor\u003e=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.8.1)\n","Requirement already satisfied: absl-py\u003e=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.10.0)\n","Requirement already satisfied: wrapt\u003e=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.12.1)\n","Requirement already satisfied: tensorboard\u003c1.15.0,\u003e=1.14.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.14.0)\n","Requirement already satisfied: google-pasta\u003e=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.2.0)\n","Requirement already satisfied: keras-applications\u003e=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.0.8)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf\u003e=3.6.1-\u003etensorflow==1.14.0) (50.3.2)\n","Requirement already satisfied: werkzeug\u003e=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard\u003c1.15.0,\u003e=1.14.0-\u003etensorflow==1.14.0) (1.0.1)\n","Requirement already satisfied: markdown\u003e=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard\u003c1.15.0,\u003e=1.14.0-\u003etensorflow==1.14.0) (3.3.3)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications\u003e=1.0.6-\u003etensorflow==1.14.0) (2.10.0)\n","Requirement already satisfied: importlib-metadata; python_version \u003c \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown\u003e=2.6.8-\u003etensorboard\u003c1.15.0,\u003e=1.14.0-\u003etensorflow==1.14.0) (2.0.0)\n","Requirement already satisfied: zipp\u003e=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version \u003c \"3.8\"-\u003emarkdown\u003e=2.6.8-\u003etensorboard\u003c1.15.0,\u003e=1.14.0-\u003etensorflow==1.14.0) (3.4.0)\n"]}],"source":["!pip install Keras==2.2.4\n","#!pip install tensorflow-gpu==1.14.0\n","!pip install tensorflow==1.14.0\n"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6381,"status":"ok","timestamp":1606163542006,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"},"user_tz":-360},"id":"cHUcIHGe22qv","outputId":"66eaf258-f6f3-4f0f-b6c7-56393526919b"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n","Using TensorFlow backend.\n"]}],"source":["import os\n","import re\n","import pickle\n","from tqdm import tqdm\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","from tensorflow.keras.layers import Dense, Dropout, Embedding, Bidirectional, LSTM\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.callbacks import TensorBoard\n","from sklearn.model_selection import train_test_split\n","# from keras.layers import Embedding\n","from  keras . utils  import  to_categorical\n","from tensorflow.keras.layers import Dense, Input, GlobalMaxPooling1D, Permute\n","from tensorflow.keras.layers import Conv1D,Conv2D, MaxPooling1D, Embedding, Flatten\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.initializers import Constant\n","from tensorflow.keras.layers import Embedding\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from keras.models import Model\n","from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n","from keras.optimizers import RMSprop\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing import sequence\n","from keras.utils import to_categorical\n","from keras.callbacks import EarlyStopping\n","%matplotlib inline\n","\n","\n","\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from keras.models import Model\n","from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n","from keras.optimizers import RMSprop\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing import sequence\n","from keras.utils import to_categorical\n","from keras.callbacks import EarlyStopping\n","%matplotlib inline"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":946,"status":"ok","timestamp":1606163548502,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"},"user_tz":-360},"id":"Y723l4kE26Z4","outputId":"d961162c-03b9-4371-cea8-748381f936fe"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17777,"status":"ok","timestamp":1606163568488,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"},"user_tz":-360},"id":"fdqmrAe626fN","outputId":"f85157f9-dcfa-4f48-bb16-4d3eb28a9e81"},"outputs":[{"name":"stdout","output_type":"stream","text":["51690\n"]}],"source":["import pandas as pd \n","df_train=pd.read_csv('gdrive/My Drive/Thesis Data/full doc csv/full_doc_train.csv')\n","df_test=pd.read_csv('gdrive/My Drive/Thesis Data/full doc csv/full_doc_test.csv')\n","total_data = pd.concat([df_train,df_test])\n","# we have to works with word level,if we works with charchter level then it exits the RAM limit\n","# here remove 'politics','education','sports' labels of text which is frequently occur in dataset and set it to news_3less.csv file\n","total_data = total_data[total_data['label']!='politics'] # remove the label which is 'politics'\n","total_data = total_data[total_data['label']!='education'] # remove the label which is 'education'\n","total_data = total_data[total_data['label']!='sports'] # remove the label which is 'sports'\n","print(len(total_data))\n","# df.to_csv(news/'news_3less.csv',index=False) # convert the data to csv format"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9955,"status":"ok","timestamp":1606163568491,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"},"user_tz":-360},"id":"TmJRNaR026y0","outputId":"eafc8301-6022-46e4-ab5f-b92dc0d654e2"},"outputs":[{"data":{"text/plain":["{'accident',\n"," 'art',\n"," 'crime',\n"," 'economics',\n"," 'entertainment',\n"," 'environment',\n"," 'international',\n"," 'opinion',\n"," 'science_tech'}"]},"execution_count":6,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["set(total_data.label)"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1005,"status":"ok","timestamp":1606163674240,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"},"user_tz":-360},"id":"RJ73rjM-26ih","outputId":"d1ffa4de-4d0c-49e9-9670-4324c33199f7"},"outputs":[{"name":"stdout","output_type":"stream","text":["fatal: destination path 'bengali-stemmer' already exists and is not an empty directory.\n"]}],"source":["!git clone -l -s https://github.com/banglakit/bengali-stemmer.git"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4886,"status":"ok","timestamp":1606163681765,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"},"user_tz":-360},"id":"kDJJUB3X26cP","outputId":"f155e5dd-ef5b-43a8-934c-f6b10cc2af69"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting git+https://github.com/banglakit/bengali-stemmer.git\n","  Cloning https://github.com/banglakit/bengali-stemmer.git to /tmp/pip-req-build-yh0npdub\n","  Running command git clone -q https://github.com/banglakit/bengali-stemmer.git /tmp/pip-req-build-yh0npdub\n","Requirement already satisfied (use --upgrade to upgrade): bengali-stemmer==0.0.1 from git+https://github.com/banglakit/bengali-stemmer.git in /usr/local/lib/python3.6/dist-packages\n","Building wheels for collected packages: bengali-stemmer\n","  Building wheel for bengali-stemmer (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for bengali-stemmer: filename=bengali_stemmer-0.0.1-py2.py3-none-any.whl size=6393 sha256=ae442534f621b938e53a1f36d8ba6e87fd9713b91fa51df3d6a250bc08750d01\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-_uz2xuiv/wheels/a1/ad/a1/4ba354059b17c00600a14e13a504e7bdb49f20f2f4e2f3639c\n","Successfully built bengali-stemmer\n"]}],"source":["!pip install git+https://github.com/banglakit/bengali-stemmer.git    "]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5439,"status":"ok","timestamp":1606163685039,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"},"user_tz":-360},"id":"EbVca--r26YU","outputId":"2f3b4c29-78b4-468f-d8c4-9a1939ddd6a7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting git+https://github.com/banglakit/bengali-stemmer.git\n","  Cloning https://github.com/banglakit/bengali-stemmer.git to /tmp/pip-req-build-30riump4\n","  Running command git clone -q https://github.com/banglakit/bengali-stemmer.git /tmp/pip-req-build-30riump4\n","Requirement already satisfied (use --upgrade to upgrade): bengali-stemmer==0.0.1 from git+https://github.com/banglakit/bengali-stemmer.git in /usr/local/lib/python3.6/dist-packages\n","Building wheels for collected packages: bengali-stemmer\n","  Building wheel for bengali-stemmer (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for bengali-stemmer: filename=bengali_stemmer-0.0.1-py2.py3-none-any.whl size=6393 sha256=51c45a5d16247f69d48cebfdde5a04f40c21754f4fbb8f1b93501f0114fe7d57\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-bokta_74/wheels/a1/ad/a1/4ba354059b17c00600a14e13a504e7bdb49f20f2f4e2f3639c\n","Successfully built bengali-stemmer\n"]}],"source":["!pip install git+https://github.com/banglakit/bengali-stemmer.git    "]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":975,"status":"ok","timestamp":1606163688867,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"},"user_tz":-360},"id":"UEPGphdd3kNS","outputId":"d994de0c-2276-4799-cc45-d2b40b42d1a4"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'বাংলা'"]},"execution_count":10,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["from bengali_stemmer.rafikamal2014 import RafiStemmer\n","stemmer = RafiStemmer()\n","stemmer.stem_word('বাংলায়')"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":1179,"status":"ok","timestamp":1606163690990,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"},"user_tz":-360},"id":"lrvjWXlw3nK5"},"outputs":[],"source":["macronum=sorted(set(total_data['label']))\n","macro_to_id = dict((note, number) for number, note in enumerate(macronum))\n","\n","def fun(i):\n","    return macro_to_id[i]\n","\n","total_data['label']=total_data['label'].apply(fun)\n"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":978,"status":"ok","timestamp":1606163693301,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"},"user_tz":-360},"id":"Gfu4TZq03zXh"},"outputs":[],"source":["texts = list(total_data['text'])\n","labels = list(total_data['label'])\n"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":1278,"status":"ok","timestamp":1606163694538,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"},"user_tz":-360},"id":"yZiIAf6532uz"},"outputs":[],"source":["def clean_punct(sentence):\n","    cleaned = re.sub(r'[?|!|\\'|\"|#|।|’|‘]', r'', sentence)\n","    cleaned1 = re.sub(r'[.|,|(|)|\\|/]', r'', cleaned)\n","    cleaned = re.sub(r'[০|১|২|৩|৪|৫|৬|৭|৮|৯]', r'', cleaned1)\n","    cleaned1 = re.sub(r'[-|=]', r' ', cleaned)\n","    return cleaned1"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":1551,"status":"ok","timestamp":1606163697784,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"},"user_tz":-360},"id":"ZEaUpOIg3596"},"outputs":[],"source":["stopwords = pd.read_csv('gdrive/My Drive/Colab Notebooks/Stopwords.csv')"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":1042,"status":"ok","timestamp":1606163699571,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"},"user_tz":-360},"id":"SUTAnoE639N5"},"outputs":[],"source":["set_stop = set(stopwords['words'])"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":968,"status":"ok","timestamp":1606163701193,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"},"user_tz":-360},"id":"nJXQG8J-4AWA"},"outputs":[],"source":["\n","def pre_process(data):\n","    i=0\n","    str1=' '\n","    final_string = []\n","    final_words = []\n","    all_negative_words = []\n","    s=''\n","\n","    for sentence in data:\n","        filtered_sentence = []\n","\n","        for w in sentence.split():\n","            for cleaned_word in clean_punct(w).split():\n","                if len(cleaned_word)\u003e2:\n","                    if((cleaned_word) not in set_stop):\n","                        s = stemmer.stem_word(cleaned_word)\n","                        if len(s)\u003e2:\n","                            final_words.append(s)\n","                            filtered_sentence.append(s)\n","                    else:\n","                        continue\n","                else:\n","                    continue\n","\n","        str1 = \" \".join(filtered_sentence)\n","        final_string.append(str1)\n","    return final_string"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":114145,"status":"ok","timestamp":1606163817869,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"},"user_tz":-360},"id":"PFo6qFob4Deh"},"outputs":[],"source":["texts = pre_process(texts)"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":120},"executionInfo":{"elapsed":1195,"status":"ok","timestamp":1606164058538,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"},"user_tz":-360},"id":"GsVSxK1b4GwR","outputId":"166b8485-fe4d-4894-b6f9-cfd7978d7a21"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'উচ্চ মাধ্যমিক পরীক্ষ মঙ্গলব বিবৃতি মন্ত্রী “আমি হরতাল অবরোধ সন্ত্রাসী কার্যক্রম পরিচালনাকারী জোট আহ্বান জানা পরীক্ষার্থী শান্তিপূর্ণভাব পরীক্ষা দয়া হটকারী ঘটনা ঘটাব “আমি ভাষা দেশ স্থান একজন পরীক্ষার্থীর ক্ষতি দায়দায়িত্ব আপনা বহন মানুষ আপনা ক্ষমা না” জানুয়ারি দেশ টানা অবরোধ চালি আসা বিএনপি জোট ফেব্রুয়ারি মার্চ বেশিরভাগ প্রতিদিন হরতাল বিএনপি জোট অবরোধ হরতাল চলতি এসএসসি সমমান অর্থাৎ দিন পরীক্ষা পেছা বাধ্য শিক্ষা মন্ত্রণাল পরীক্ষা দিন শুক্র শনিব ধরন রাজনৈতিক কর্মসূচির কারণ এইচএসসি পরীক্ষা পেছানো রেখ শিক্ষামন্ত্রী ফাইল বিবৃতি নাহিদ “সংকট এসএসসি পরীক্ষা শেষ এইচএসসি পরীক্ষা জাতির দুর্ভাগ্য রাজনৈতিক জোট বিবেকবর্জিত অব্যাহত হরতাল অবরোধ কারণ এসএসসির রুটিন পরীক্ষা সম্ভব “পরীক্ষার্থী অভিভাবক শিক্ষকসহ শ্রেণি পেশ মানুষ মতামত পরামর্শ সম্মান দেখি পরিস্থিতি রুটিনমাফিক পরীক্ষা সিদ্ধান্ত গ্রহণ করে ব্যত্য না” পরীক্ষার্থী যাতায়াত নির্বিঘ্ন আইনশৃঙ্খলা রক্ষাবাহিনীর সর্বাত্মক নজরদারি আশ্বস্ত শিক্ষামন্ত্রী “ইতিবাচক দৃষ্টিভঙ্গিসম্পন্ন রাজনৈতিক নেতা কর্মী সামাজিক সাংস্কৃতিক সংগঠন সদস্য জনগণ তোমা পাশ তোম নিশ্চিন্ত পরীক্ষা দেবে” প্রশ্ন ফাঁস গুজব শিক্ষার্থী অভিভাবক বিভ্রান্ত আহ্বান পরীক্ষার্থী লেখাপড়া পরামর্শ শিক্ষামন্ত্রী অবরোধ এইচএসসি সমমান পরীক্ষা বসছ লাখ শিক্ষার্থী এপ্রিল জুন এইচএসসি সমমান তত্ত্বী বিষয় পরীক্ষা ব্যবহারিক পরীক্ষা জুন জুন'"]},"execution_count":18,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["texts[0]"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":959,"status":"ok","timestamp":1606164059995,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"},"user_tz":-360},"id":"eHpamuBc4KKf"},"outputs":[],"source":["def load_data(num_words, sequence_length, test_size=0.25, oov_token=None):\n","    tokenizer = Tokenizer(num_words=num_words, oov_token=oov_token)\n","    num_classes = 9 \n","    y = to_categorical(np.asarray(labels), num_classes)\n","    tokenizer.fit_on_texts(texts)\n","    X = tokenizer.texts_to_sequences(texts)\n","    X = np.array(X)\n","    # pad sequences with 0's\n","    X = pad_sequences(X, maxlen=sequence_length)\n","    # split data to training and testing sets\n","    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=1)\n","    data = {}\n","    data[\"X_train\"] = X_train\n","    data[\"X_test\"]= X_test\n","    data[\"y_train\"] = y_train\n","    data[\"y_test\"] = y_test\n","    data[\"tokenizer\"] = tokenizer\n","    return data"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":952,"status":"ok","timestamp":1606164063648,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"},"user_tz":-360},"id":"GSLffpPo47Dk"},"outputs":[],"source":["# train_data=pd.read_csv('gdrive/My Drive/Colab Notebooks/ulm_train.csv')\n","import numpy as np\n","\n","def get_embedding_vectors(word_index, embedding_size=100):\n","    \n","    embedding_matrix = np.zeros((len(word_index) + 1, embedding_size))\n","    with open(f\"gdrive/My Drive/Colab Notebooks/bn_glove.{embedding_size}d.txt\", encoding=\"utf8\") as f:\n","        for line in tqdm(f, \"Reading GloVe\"):\n","            values = line.split()\n","            # get the word as the first word in the line\n","            word = values[0]\n","            if word in word_index:\n","                idx = word_index[word]\n","                # get the vectors as the remaining values in the line\n","                embedding_matrix[idx] = np.array(values[1:], dtype=\"float32\")\n","    return embedding_matrix"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":3212,"status":"ok","timestamp":1606164069139,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"},"user_tz":-360},"id":"X_RIYc1l4KNs"},"outputs":[],"source":["d = set()\n","for s in texts:\n","    for ss in list(s.split()):\n","        d.add(ss)"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":988,"status":"ok","timestamp":1606164071020,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"},"user_tz":-360},"id":"-IlWYOky4UcA","outputId":"abf4d59d-0a6b-47f2-8a68-93ea9db0d886"},"outputs":[{"data":{"text/plain":["278693"]},"execution_count":22,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["len(d) # here is the total number of words in corpus, but the number of tokenizer is small then that(ther are some stopwords which is removed)"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1637,"status":"ok","timestamp":1606164073724,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"},"user_tz":-360},"id":"ha5Rg0dT4UXc","outputId":"614525b5-8db3-49b6-bd3e-8251448cec2c"},"outputs":[{"data":{"text/plain":["7779"]},"execution_count":23,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["max([len(s.split()) for s in texts]) # find the max length"]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":24297,"status":"ok","timestamp":1606164098714,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"},"user_tz":-360},"id":"jEamF9J-4UWL"},"outputs":[],"source":["# MAX_SEQUENCE_LENGTH = 7779    #max([len(s.split()) for s in texts]) \n","max_len = 1500\n","MAX_NUM_WORDS = 272611 + 1 # you have to use word same as tokenizer length, and add extra 1(for 0th index case)\n","vocab_size = MAX_NUM_WORDS\n","EMBEDDING_DIM = 300\n","VALIDATION_SPLIT = 0.2\n","\n","data = load_data(MAX_NUM_WORDS , max_len,VALIDATION_SPLIT)"]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22357,"status":"ok","timestamp":1606164098716,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"},"user_tz":-360},"id":"L4FjGyqx4fm7","outputId":"9267271e-6018-4a08-bc2b-af4f1c05ba27"},"outputs":[{"data":{"text/plain":["272611"]},"execution_count":25,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["len(data['tokenizer'].word_index) # after running previous cell of code we run this cell,we call the 'load_data' function before we see the len of data['tokenizer']"]},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11959,"status":"ok","timestamp":1606164112920,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"},"user_tz":-360},"id":"J0ut6dkk5CxP","outputId":"c9f70863-90b6-40ce-c9d1-56028da5b497"},"outputs":[{"name":"stderr","output_type":"stream","text":["Reading GloVe: 134256it [00:10, 12646.37it/s]\n"]}],"source":["embedding_matrix = get_embedding_vectors( data['tokenizer'].word_index ,EMBEDDING_DIM )"]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":909,"status":"ok","timestamp":1606164114779,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"},"user_tz":-360},"id":"EXMHoPay4foo"},"outputs":[],"source":["max_words = 272611 + 1\n","# max_len = 7779 \n","# tok = Tokenizer(num_words=max_words)\n","# tok.fit_on_texts(X_train)\n","# sequences = tok.texts_to_sequences(X_train)\n","# sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)"]},{"cell_type":"code","execution_count":28,"metadata":{"executionInfo":{"elapsed":909,"status":"ok","timestamp":1606164117927,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"},"user_tz":-360},"id":"5EoNJAUM4fsq"},"outputs":[],"source":["from keras import backend as K\n","from keras.engine.topology import Layer\n","from keras import initializers, regularizers, constraints\n","\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU \n","from keras.layers import GRU, LSTM, BatchNormalization\n","from keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten\n","from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\n","from keras.models import Model, load_model\n","from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n","from keras import backend as K\n","from keras.engine import InputSpec, Layer\n","from keras.optimizers import Adam\n","\n","from keras.callbacks import ModelCheckpoint, TensorBoard, Callback, EarlyStopping"]},{"cell_type":"code","execution_count":29,"metadata":{"executionInfo":{"elapsed":915,"status":"ok","timestamp":1606164120990,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"},"user_tz":-360},"id":"qFZT7iyP4fi-"},"outputs":[],"source":["class Attention(Layer):\n","    \"\"\"\n","    Keras Layer that implements an Attention mechanism for temporal data.\n","    Supports Masking.\n","    Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n","    # Input shape\n","        3D tensor with shape: `(samples, steps, features)`.\n","    # Output shape\n","        2D tensor with shape: `(samples, features)`.\n","    :param kwargs:\n","    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n","    The dimensions are inferred based on the output shape of the RNN.\n","    Example:\n","        model.add(LSTM(64, return_sequences=True))\n","        model.add(Attention())\n","    \"\"\"   \n","   \n","   \n","    def __init__(self, step_dim,\n","                 W_regularizer=None, b_regularizer=None,\n","                 W_constraint=None, b_constraint=None,\n","                 bias=True, **kwargs):\n","        self.supports_masking = True\n","        self.init = initializers.get('glorot_uniform')\n","\n","        self.W_regularizer = regularizers.get(W_regularizer)\n","        self.b_regularizer = regularizers.get(b_regularizer)\n","\n","        self.W_constraint = constraints.get(W_constraint)\n","        self.b_constraint = constraints.get(b_constraint)\n","\n","        self.bias = bias\n","        self.step_dim = step_dim\n","        self.features_dim = 0\n","        super(Attention, self).__init__(**kwargs)          \n","            \n","            \n","        self.features_dim = 0\n","        super(Attention, self).__init__(**kwargs)\n","\n","    def build(self, input_shape):\n","        assert len(input_shape) == 3\n","\n","        self.W = self.add_weight((input_shape[-1],),\n","                                 initializer=self.init,\n","                                 name='{}_W'.format(self.name),\n","                                 regularizer=self.W_regularizer,\n","                                 constraint=self.W_constraint)\n","        self.features_dim = input_shape[-1]\n","\n","        if self.bias:\n","            self.b = self.add_weight((input_shape[1],),\n","                                     initializer='zero',\n","                                     name='{}_b'.format(self.name),\n","                                     regularizer=self.b_regularizer,\n","                                     constraint=self.b_constraint)\n","        else:\n","            self.b = None\n","        self.built = True\n","    \n","    def compute_mask(self, input, input_mask=None):\n","        return None\n","\n","    def call(self, x, mask=None):\n","        features_dim = self.features_dim\n","        step_dim = self.step_dim\n","        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n","                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n","        if self.bias:\n","            eij += self.b\n","        eij = K.tanh(eij)\n","        a = K.exp(eij)\n","        if mask is not None:\n","            a *= K.cast(mask, K.floatx())\n","        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n","        a = K.expand_dims(a)\n","        weighted_input = x * a\n","        return K.sum(weighted_input, axis=1)\n","\n","    def compute_output_shape(self, input_shape):\n","        return input_shape[0],  self.features_dim"]},{"cell_type":"markdown","metadata":{"id":"S3qAi77G8I6t"},"source":["max_len = 2500, bs=64"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":837},"executionInfo":{"elapsed":3265,"status":"ok","timestamp":1606164129021,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"},"user_tz":-360},"id":"Om0tk1nl4KIr","outputId":"ffa34862-c805-40a7-cd11-859a826737c4"},"outputs":[{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2974: add_dispatch_support.\u003clocals\u003e.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_1 (InputLayer)         (None, 1500)              0         \n","_________________________________________________________________\n","embedding_1 (Embedding)      (None, 1500, 300)         81783600  \n","_________________________________________________________________\n","spatial_dropout1d_1 (Spatial (None, 1500, 300)         0         \n","_________________________________________________________________\n","bidirectional_1 (Bidirection (None, 1500, 256)         439296    \n","_________________________________________________________________\n","bidirectional_2 (Bidirection (None, 1500, 256)         394240    \n","_________________________________________________________________\n","attention_2 (Attention)      (None, 256)               1756      \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 256)               0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 64)                16448     \n","_________________________________________________________________\n","dropout_2 (Dropout)          (None, 64)                0         \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 9)                 585       \n","=================================================================\n","Total params: 82,635,925\n","Trainable params: 852,325\n","Non-trainable params: 81,783,600\n","_________________________________________________________________\n"]},{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\nMAX_SEQUENCE_LENGTH = 11276    #max([len(s.split()) for s in texts]) \\nMAX_NUM_WORDS = 109803 + 1\\nvocab_size = MAX_NUM_WORDS\\nEMBEDDING_DIM = 300\\nVALIDATION_SPLIT = 0.2\\n\\ndata = load_data(MAX_NUM_WORDS , MAX_SEQUENCE_LENGTH,VALIDATION_SPLIT)\\n'"]},"execution_count":30,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["max_features = 272611 + 1\n","def build_model(max_len, max_features, EMBEDDING_DIM, embedding_matrix):\n","    input_words = Input((max_len, ))\n","    x_words = Embedding(max_features,\n","                        EMBEDDING_DIM,\n","                        weights=[embedding_matrix],\n","                        mask_zero=True,\n","                        trainable=False)(input_words)\n","    x_words = SpatialDropout1D(0.2)(x_words)\n","    x_words = Bidirectional(LSTM(128, return_sequences=True))(x_words)\n","    x_words = Bidirectional(LSTM(128, return_sequences=True))(x_words)\n","    \n","    x = Attention(max_len)(x_words)\n","    #x = GlobalMaxPooling1D()(x)\n","    #x = GlobalAveragePooling1D()(x)\n","    x = Dropout(0.2)(x)\n","    x = Dense(64, activation='relu')(x)\n","    x = Dropout(0.2)(x)\n","    pred = Dense(9, activation='softmax')(x)\n","\n","    model = Model(inputs=input_words, outputs=pred)\n","    return model\n","\n","model = build_model(max_len, max_features, EMBEDDING_DIM, embedding_matrix)\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","model.summary()\n","'''\n","MAX_SEQUENCE_LENGTH = 11276    #max([len(s.split()) for s in texts]) \n","MAX_NUM_WORDS = 109803 + 1\n","vocab_size = MAX_NUM_WORDS\n","EMBEDDING_DIM = 300\n","VALIDATION_SPLIT = 0.2\n","\n","data = load_data(MAX_NUM_WORDS , MAX_SEQUENCE_LENGTH,VALIDATION_SPLIT)\n","'''"]},{"cell_type":"code","execution_count":31,"metadata":{"executionInfo":{"elapsed":982,"status":"ok","timestamp":1606164135347,"user":{"displayName":"ibrahim al azhar","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhcTQUbimVsO7p7wRuyYZSNKei7erJoiKLzcwQ3eoY=s64","userId":"00085534427974934913"},"user_tz":-360},"id":"XIDUZzFK8mqw"},"outputs":[],"source":["import tensorflow as tf\n","callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"4Lhh0mLV8mmK"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train on 41352 samples, validate on 10338 samples\n","Epoch 1/10\n"]},{"ename":"KeyboardInterrupt","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-32-680344e11836\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# use only 10 epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 2\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'X_train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y_train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'X_test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y_test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# use only 10 epochs\n","history = model.fit(data['X_train'], data['y_train'], validation_data=(data['X_test'], data['y_test']), epochs=10, verbose=2, batch_size=128, callbacks=[callback])"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMceANmexM/q+QTW+aY/ZNN","collapsed_sections":[],"name":"ATTENTION with LSTM main 9 news.ipynb","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}